{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 17:45:18.922817: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-22 17:45:18.965797: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-22 17:45:18.965828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-22 17:45:18.968292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-22 17:45:18.977777: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-22 17:45:18.978514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-22 17:45:19.875046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(3407)\n",
    "np.random.seed(3407)\n",
    "random.seed(3407)\n",
    "\n",
    "from notebooks.helper_functions import (\n",
    "    convert_bytes,\n",
    "    convert_prefetchdataset_to_numpy_arrays,\n",
    "    evaluate_time_of_prediction,\n",
    "    get_file_size,\n",
    "    predict_and_print_full_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11292 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 17:45:36.087331: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-22 17:45:36.087784: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-09-22 17:45:36.161771: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2024-09-22 17:45:36.168977: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "Found 1393 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "Found 1380 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "Classes:  ['non_target' 'target']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.audio_dataset_from_directory(\n",
    "    \"../../dataset/training\",\n",
    "    labels=\"inferred\",\n",
    "    sampling_rate=16000,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=3407,\n",
    ")\n",
    "test_dataset = tf.keras.utils.audio_dataset_from_directory(\n",
    "    \"../../dataset/testing\",\n",
    "    labels=\"inferred\",\n",
    "    sampling_rate=16000,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=3407,\n",
    ")\n",
    "val_dataset = tf.keras.utils.audio_dataset_from_directory(\n",
    "    \"../../dataset/validation\",\n",
    "    labels=\"inferred\",\n",
    "    sampling_rate=16000,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=3407,\n",
    ")\n",
    "\n",
    "label_names = np.array(train_dataset.class_names)\n",
    "print(\"Classes: \", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np, y_train_np = convert_prefetchdataset_to_numpy_arrays(\n",
    "    train_dataset, data_type=\"time-series\"\n",
    ")\n",
    "x_val_np, y_val_np = convert_prefetchdataset_to_numpy_arrays(\n",
    "    val_dataset, data_type=\"time-series\"\n",
    ")\n",
    "x_test_np, y_test_np = convert_prefetchdataset_to_numpy_arrays(\n",
    "    test_dataset, data_type=\"time-series\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes, num_of_layers):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.num_of_conv_layers = num_of_layers\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Hyperparameters\n",
    "        kernel_size = hp.Choice(\"kernel_size\", values=[2, 3, 5])\n",
    "        dense_units = hp.Int(\"1st_dense_units\", min_value=4, max_value=32, step=4)\n",
    "        dense_activation = hp.Choice(\n",
    "            \"2nd_dense_activation\", values=[\"softmax\", \"sigmoid\"]\n",
    "        )\n",
    "        learning_rate = hp.Float(\n",
    "            \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n",
    "        )\n",
    "\n",
    "        # Model architecture\n",
    "        model.add(tf.keras.layers.Input(shape=self.input_shape))\n",
    "\n",
    "        # Tune number of layers\n",
    "        for i in range(self.num_of_conv_layers):\n",
    "            model.add(\n",
    "                tf.keras.layers.Conv1D(\n",
    "                    filters=hp.Int(f\"filters_{i}\", min_value=2, max_value=8, step=2),\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation=hp.Choice(\"conv2d_activation\", [\"relu\", \"tanh\"]),\n",
    "                )\n",
    "            )\n",
    "            model.add(\n",
    "                tf.keras.layers.MaxPooling1D(\n",
    "                    hp.Int(\"pull_size\", min_value=4, max_value=10, step=2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(units=dense_units, activation=\"relu\"))\n",
    "\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(units=self.num_classes, activation=dense_activation)\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 1 conv2d layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 07m 29s]\n",
      "val_accuracy: 0.9322463870048523\n",
      "\n",
      "Best val_accuracy So Far: 0.9615941941738129\n",
      "Total elapsed time: 01h 14m 15s\n",
      "Results summary\n",
      "Results in hpo_tuner/cnn/cnn_time_series_1_conv1d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 20\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.00392551860940815\n",
      "filters_0: 4\n",
      "conv2d_activation: relu\n",
      "pull_size: 8\n",
      "Score: 0.9615941941738129\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 16\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0018228072765774177\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 6\n",
      "Score: 0.9550724625587463\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.00458502351256499\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 4\n",
      "Score: 0.9503623247146606\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.00019705017081231895\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 10\n",
      "Score: 0.9434782862663269\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 8\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0004703116290931317\n",
      "filters_0: 2\n",
      "conv2d_activation: tanh\n",
      "pull_size: 4\n",
      "Score: 0.9409420192241669\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 20\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.004424632678209153\n",
      "filters_0: 2\n",
      "conv2d_activation: tanh\n",
      "pull_size: 6\n",
      "Score: 0.9376811683177948\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 12\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.009039164194258226\n",
      "filters_0: 6\n",
      "conv2d_activation: relu\n",
      "pull_size: 6\n",
      "Score: 0.9326086938381195\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 32\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.00021363672907531923\n",
      "filters_0: 8\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "Score: 0.9322463870048523\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 8\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.002326083256745929\n",
      "filters_0: 4\n",
      "conv2d_activation: relu\n",
      "pull_size: 6\n",
      "Score: 0.907608687877655\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.009807310652433296\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 4\n",
      "Score: 0.8833333551883698\n"
     ]
    }
   ],
   "source": [
    "input_shape = (48000, 1)\n",
    "num_classes = 2\n",
    "\n",
    "num_of_conv_layers = 1\n",
    "tuner_1_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_conv_layers),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"hpo_tuner/cnn\",\n",
    "    project_name=\"cnn_time_series_1_conv1d_tuning\",\n",
    ")\n",
    "tuner_1_layer_cnn.search(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "tuner_1_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 2 conv2d layers CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 07m 25s]\n",
      "val_accuracy: 0.8014492690563202\n",
      "\n",
      "Best val_accuracy So Far: 0.969565212726593\n",
      "Total elapsed time: 01h 10m 33s\n",
      "Results summary\n",
      "Results in hpo_tuner/cnn/cnn_time_series_2_conv1d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 16\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.008792598590199204\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 6\n",
      "Score: 0.969565212726593\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 4\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.008002917774687932\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 8\n",
      "filters_1: 4\n",
      "Score: 0.9420289993286133\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 8\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0003233648014907526\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 6\n",
      "filters_1: 4\n",
      "Score: 0.9402174055576324\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 12\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0008640259802512754\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 6\n",
      "filters_1: 2\n",
      "Score: 0.9402173757553101\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 4\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0017921947583188819\n",
      "filters_0: 2\n",
      "conv2d_activation: tanh\n",
      "pull_size: 8\n",
      "filters_1: 2\n",
      "Score: 0.9351449310779572\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 20\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0008385828184982733\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 8\n",
      "filters_1: 2\n",
      "Score: 0.9336956739425659\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0016718413174765733\n",
      "filters_0: 4\n",
      "conv2d_activation: relu\n",
      "pull_size: 4\n",
      "filters_1: 2\n",
      "Score: 0.9322463870048523\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 4\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0031645234744137724\n",
      "filters_0: 4\n",
      "conv2d_activation: relu\n",
      "pull_size: 6\n",
      "filters_1: 4\n",
      "Score: 0.9231884181499481\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.00018527050551082463\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 2\n",
      "Score: 0.9126811623573303\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 4\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.000742273365279094\n",
      "filters_0: 8\n",
      "conv2d_activation: relu\n",
      "pull_size: 8\n",
      "filters_1: 8\n",
      "Score: 0.8014492690563202\n"
     ]
    }
   ],
   "source": [
    "num_of_layers = 2\n",
    "tuner_2_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_layers),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"hpo_tuner/cnn\",\n",
    "    project_name=\"cnn_time_series_2_conv1d_tuning\",\n",
    ")\n",
    "tuner_2_layer_cnn.search(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "tuner_2_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 3 conv2d layers CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 07m 06s]\n",
      "val_accuracy: 0.968478262424469\n",
      "\n",
      "Best val_accuracy So Far: 0.9771738946437836\n",
      "Total elapsed time: 01h 15m 42s\n",
      "Results summary\n",
      "Results in hpo_tuner/cnn/cnn_time_series_3_conv1d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 12\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0011775930616089965\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 8\n",
      "filters_1: 4\n",
      "filters_2: 8\n",
      "Score: 0.9771738946437836\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 8\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0011016692761180362\n",
      "filters_0: 2\n",
      "conv2d_activation: relu\n",
      "pull_size: 10\n",
      "filters_1: 6\n",
      "filters_2: 6\n",
      "Score: 0.9702898561954498\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 32\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0071544516667882965\n",
      "filters_0: 2\n",
      "conv2d_activation: tanh\n",
      "pull_size: 6\n",
      "filters_1: 4\n",
      "filters_2: 2\n",
      "Score: 0.968478262424469\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 28\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0004542815732661726\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 2\n",
      "filters_2: 6\n",
      "Score: 0.9605072438716888\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0002841643844798154\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 4\n",
      "filters_1: 8\n",
      "filters_2: 8\n",
      "Score: 0.9590579569339752\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 32\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.00011201789770905872\n",
      "filters_0: 6\n",
      "conv2d_activation: relu\n",
      "pull_size: 10\n",
      "filters_1: 4\n",
      "filters_2: 8\n",
      "Score: 0.9420289695262909\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 20\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.001788114586587675\n",
      "filters_0: 2\n",
      "conv2d_activation: relu\n",
      "pull_size: 4\n",
      "filters_1: 2\n",
      "filters_2: 4\n",
      "Score: 0.9416666924953461\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 32\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0006382243288146399\n",
      "filters_0: 6\n",
      "conv2d_activation: relu\n",
      "pull_size: 4\n",
      "filters_1: 2\n",
      "filters_2: 2\n",
      "Score: 0.9398550689220428\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 12\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.00030566563438515443\n",
      "filters_0: 6\n",
      "conv2d_activation: tanh\n",
      "pull_size: 4\n",
      "filters_1: 2\n",
      "filters_2: 4\n",
      "Score: 0.9300724864006042\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 8\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.00010278546049636793\n",
      "filters_0: 6\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 2\n",
      "filters_2: 6\n",
      "Score: 0.7760869562625885\n"
     ]
    }
   ],
   "source": [
    "num_of_layers = 3\n",
    "tuner_3_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_layers),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"hpo_tuner/cnn\",\n",
    "    project_name=\"cnn_time_series_3_conv1d_tuning\",\n",
    ")\n",
    "tuner_3_layer_cnn.search(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "tuner_3_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "pull_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 2, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner_1_layer_cnn.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "pull_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 2, 'sampling': 'linear'}\n",
      "filters_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner_2_layer_cnn.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "pull_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 10, 'step': 2, 'sampling': 'linear'}\n",
      "filters_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "filters_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner_3_layer_cnn.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 16\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.008792598590199204\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 6\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_2_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ... models, that showed good (or best) results in HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 01 summary\n",
    "# Hyperparameters:\n",
    "# kernel_size: 5\n",
    "# 1st_dense_units: 20\n",
    "# 2nd_dense_activation: sigmoid\n",
    "# learning_rate: 0.00392551860940815\n",
    "# filters_0: 4\n",
    "# conv2d_activation: relu\n",
    "# pull_size: 8\n",
    "# Score: 0.9615941941738129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_8 (Conv1D)           (None, 47996, 4)          24        \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPoolin  (None, 5999, 4)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 23996)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 20)                479940    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480006 (1.83 MB)\n",
      "Trainable params: 480006 (1.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(48000, 1)),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=5, activation=\"relu\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(20, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 55s 150ms/step - loss: 0.3176 - accuracy: 0.8649\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 55s 153ms/step - loss: 0.2253 - accuracy: 0.9074\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 57s 160ms/step - loss: 0.1674 - accuracy: 0.9381\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 54s 151ms/step - loss: 0.1525 - accuracy: 0.9423\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 55s 152ms/step - loss: 0.1257 - accuracy: 0.9538\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 55s 154ms/step - loss: 0.0947 - accuracy: 0.9653\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 54s 151ms/step - loss: 0.0805 - accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 54s 150ms/step - loss: 0.0948 - accuracy: 0.9681\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 57s 158ms/step - loss: 0.0777 - accuracy: 0.9740\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 54s 150ms/step - loss: 0.0622 - accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00392551860940815),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "# EPOCHS = 20\n",
    "EPOCHS = 10\n",
    "history = model_1.fit(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.2669 - accuracy: 0.9435\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_1.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 32ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 94.20%\n",
      "Recall: 87.29%\n",
      "Precision: 95.37%\n",
      "F1-score: 91.15%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9109103035957894\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9102927859588559\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 2s 36ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 94.54%\n",
      "Recall: 90.61%\n",
      "Precision: 92.63%\n",
      "F1-score: 91.61%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9155301305164129\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9137298632059566\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 94.54%\n",
      "Recall: 90.61%\n",
      "Precision: 92.63%\n",
      "F1-score: 91.61%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.075 seconds\n",
      "Max: 0.163 seconds\n",
      "Min: 0.053 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val,\n",
    "    non_overlap_patritions_f1_scores_val,\n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_1, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test,\n",
    "    non_overlap_patritions_f1_scores_test,\n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_1, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(\n",
    "    model_1, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  spectrogram_models_from_notebooks/cnn/hpo/cnn_time_ser_1_conv_layer_model.keras\n",
      "File size: 5.522 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = (\n",
    "    \"time_series_models_from_notebooks/cnn/hpo/cnn_time_ser_1_conv_layer_model.keras\"\n",
    ")\n",
    "model_1.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer CNN is too big. We dont have second MaxPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained this model for 5 and for 10 epochs. Less epochs (5) give better F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 16\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.008792598590199204\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 10\n",
      "filters_1: 6\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_2_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")\n",
    "\n",
    "# Trial 05 summary\n",
    "# Hyperparameters:\n",
    "# kernel_size: 5\n",
    "# 1st_dense_units: 8\n",
    "# 2nd_dense_activation: softmax\n",
    "# learning_rate: 0.0011016692761180362\n",
    "# filters_0: 2\n",
    "# conv2d_activation: relu\n",
    "# pull_size: 10\n",
    "# filters_1: 6\n",
    "# filters_2: 6\n",
    "# Score: 0.9702898561954498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_13 (Conv1D)          (None, 47999, 4)          12        \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPooli  (None, 4799, 4)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 4798, 6)           54        \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPooli  (None, 479, 6)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2874)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                46000     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46100 (180.08 KB)\n",
      "Trainable params: 46100 (180.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 41s 113ms/step - loss: 0.2954 - accuracy: 0.8604\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 41s 114ms/step - loss: 0.1639 - accuracy: 0.9401\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 50s 139ms/step - loss: 0.1350 - accuracy: 0.9524\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 50s 138ms/step - loss: 0.1053 - accuracy: 0.9643\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 50s 140ms/step - loss: 0.1048 - accuracy: 0.9649\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 48s 133ms/step - loss: 0.0864 - accuracy: 0.9703\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 47s 131ms/step - loss: 0.0715 - accuracy: 0.9762\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 48s 134ms/step - loss: 0.0628 - accuracy: 0.9796\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 48s 134ms/step - loss: 0.0709 - accuracy: 0.9752\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 47s 131ms/step - loss: 0.0644 - accuracy: 0.9779\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 2s 30ms/step - loss: 0.1134 - accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "model_2_10_epochs = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(48000, 1)),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=2, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=10),\n",
    "        keras.layers.Conv1D(filters=6, kernel_size=2, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=10),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_2_10_epochs.summary()\n",
    "\n",
    "model_2_10_epochs.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.008792598590199204),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 10\n",
    "# EPOCHS = 20\n",
    "history = model_2_10_epochs.fit(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_2_10_epochs.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 27ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 96.88%\n",
      "Recall: 98.09%\n",
      "Precision: 93.16%\n",
      "F1-score: 95.56%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9559573585383309\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9624568754092924\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 26ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.62%\n",
      "Recall: 96.94%\n",
      "Precision: 90.43%\n",
      "F1-score: 93.57%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9351237580868015\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.938288538065416\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 95.62%\n",
      "Recall: 96.94%\n",
      "Precision: 90.43%\n",
      "F1-score: 93.57%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.08 seconds\n",
      "Max: 1.108 seconds\n",
      "Min: 0.055 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val,\n",
    "    non_overlap_patritions_f1_scores_val,\n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(\n",
    "    model_2_10_epochs, x_val_np, y_val_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test,\n",
    "    non_overlap_patritions_f1_scores_test,\n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(\n",
    "    model_2_10_epochs, x_test_np, y_test_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(\n",
    "    model_2_10_epochs,\n",
    "    x_test_np,\n",
    "    y_test_np,\n",
    "    model_format=\"keras\",\n",
    "    show_prediction_evaluation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 46s 126ms/step - loss: 0.2764 - accuracy: 0.8801\n",
      "Epoch 2/5\n",
      "353/353 [==============================] - 47s 130ms/step - loss: 0.1706 - accuracy: 0.9350\n",
      "Epoch 3/5\n",
      "353/353 [==============================] - 47s 129ms/step - loss: 0.1277 - accuracy: 0.9518\n",
      "Epoch 4/5\n",
      "353/353 [==============================] - 50s 140ms/step - loss: 0.1080 - accuracy: 0.9602\n",
      "Epoch 5/5\n",
      "353/353 [==============================] - 51s 141ms/step - loss: 0.0943 - accuracy: 0.9682\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 2s 30ms/step - loss: 0.1074 - accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.008792598590199204),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 5\n",
    "# EPOCHS = 20\n",
    "history = model_2.fit(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_2.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 28ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 96.38%\n",
      "Recall: 95.13%\n",
      "Precision: 94.33%\n",
      "F1-score: 94.73%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9471310832487327\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9506330911683708\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 30ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 96.63%\n",
      "Recall: 95.85%\n",
      "Precision: 94.00%\n",
      "F1-score: 94.92%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9482034905815224\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9468597200306423\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 96.63%\n",
      "Recall: 95.85%\n",
      "Precision: 94.00%\n",
      "F1-score: 94.92%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.075 seconds\n",
      "Max: 0.132 seconds\n",
      "Min: 0.056 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val,\n",
    "    non_overlap_patritions_f1_scores_val,\n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_2, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test,\n",
    "    non_overlap_patritions_f1_scores_test,\n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_2, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(\n",
    "    model_2, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  spectrogram_models_from_notebooks/cnn/hpo/cnn_time_ser_2_conv_layer_model.keras\n",
      "File size: 0.563 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = (\n",
    "    \"time_series_models_from_notebooks/cnn/hpo/cnn_time_ser_2_conv_layer_model.keras\"\n",
    ")\n",
    "model_2.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 12\n",
      "2nd_dense_activation: sigmoid\n",
      "learning_rate: 0.0011775930616089965\n",
      "filters_0: 4\n",
      "conv2d_activation: tanh\n",
      "pull_size: 8\n",
      "filters_1: 4\n",
      "filters_2: 8\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_3_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 47996, 4)          24        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 5999, 4)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 5995, 4)           84        \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 749, 4)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 745, 8)            168       \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 93, 8)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 744)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                8940      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9242 (36.10 KB)\n",
      "Trainable params: 9242 (36.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3_5_epochs = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(48000, 1)),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Conv1D(filters=8, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(12, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_3_5_epochs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 41s 112ms/step - loss: 0.2602 - accuracy: 0.8858\n",
      "Epoch 2/5\n",
      "353/353 [==============================] - 41s 115ms/step - loss: 0.1081 - accuracy: 0.9640\n",
      "Epoch 3/5\n",
      "353/353 [==============================] - 43s 121ms/step - loss: 0.0862 - accuracy: 0.9723\n",
      "Epoch 4/5\n",
      "353/353 [==============================] - 43s 120ms/step - loss: 0.0679 - accuracy: 0.9782\n",
      "Epoch 5/5\n",
      "353/353 [==============================] - 42s 118ms/step - loss: 0.0621 - accuracy: 0.9794\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0615 - accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "model_3_5_epochs.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0011775930616089965),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 5\n",
    "# EPOCHS = 10\n",
    "history = model_3_5_epochs.fit(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_3_5_epochs.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 27ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 98.19%\n",
      "Recall: 97.25%\n",
      "Precision: 97.45%\n",
      "F1-score: 97.35%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9733570239972023\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9749159656636003\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 28ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 98.56%\n",
      "Recall: 97.16%\n",
      "Precision: 98.45%\n",
      "F1-score: 97.80%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9775628286814755\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9757424045006187\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 98.56%\n",
      "Recall: 97.16%\n",
      "Precision: 98.45%\n",
      "F1-score: 97.80%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.048 seconds\n",
      "Max: 0.194 seconds\n",
      "Min: 0.043 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val,\n",
    "    non_overlap_patritions_f1_scores_val,\n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(\n",
    "    model_3_5_epochs, x_val_np, y_val_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test,\n",
    "    non_overlap_patritions_f1_scores_test,\n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(\n",
    "    model_3_5_epochs, x_test_np, y_test_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(\n",
    "    model_3_5_epochs,\n",
    "    x_test_np,\n",
    "    y_test_np,\n",
    "    model_format=\"keras\",\n",
    "    show_prediction_evaluation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 47996, 4)          24        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 5999, 4)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 5995, 4)           84        \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 749, 4)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 745, 8)            168       \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 93, 8)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 744)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                8940      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9242 (36.10 KB)\n",
      "Trainable params: 9242 (36.10 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3_10_epochs = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(48000, 1)),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Conv1D(filters=4, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Conv1D(filters=8, kernel_size=5, activation=\"tanh\"),\n",
    "        keras.layers.MaxPooling1D(pool_size=8),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(12, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_3_10_epochs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 56s 153ms/step - loss: 0.2602 - accuracy: 0.8858\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 53s 148ms/step - loss: 0.1081 - accuracy: 0.9640\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 62s 174ms/step - loss: 0.0862 - accuracy: 0.9723\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 69s 193ms/step - loss: 0.0679 - accuracy: 0.9782\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 75s 208ms/step - loss: 0.0621 - accuracy: 0.9794\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 63s 176ms/step - loss: 0.0495 - accuracy: 0.9824\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 63s 174ms/step - loss: 0.0445 - accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 62s 172ms/step - loss: 0.0358 - accuracy: 0.9871\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 62s 172ms/step - loss: 0.0310 - accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 61s 170ms/step - loss: 0.0322 - accuracy: 0.9882\n",
      "Validation dataset accuracy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 17:57:43.999459: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 264960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 2s 39ms/step - loss: 0.0657 - accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "model_3_10_epochs.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0011775930616089965),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 10\n",
    "history = model_3_10_epochs.fit(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_3_10_epochs.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      " 1/44 [..............................] - ETA: 6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 18:01:54.874296: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 264960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 2s 39ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 98.33%\n",
      "Recall: 97.25%\n",
      "Precision: 97.87%\n",
      "F1-score: 97.56%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9753312320782165\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9769318421377577\n",
      "\n",
      "Test dataset:\n",
      " 1/44 [..............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 18:02:25.346150: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 267456000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 3s 70ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 98.85%\n",
      "Recall: 97.82%\n",
      "Precision: 98.68%\n",
      "F1-score: 98.25%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9817988410806853\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9810600944409472\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 98.85%\n",
      "Recall: 97.82%\n",
      "Precision: 98.68%\n",
      "F1-score: 98.25%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.078 seconds\n",
      "Max: 0.456 seconds\n",
      "Min: 0.044 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val,\n",
    "    non_overlap_patritions_f1_scores_val,\n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(\n",
    "    model_3_10_epochs, x_val_np, y_val_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test,\n",
    "    non_overlap_patritions_f1_scores_test,\n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(\n",
    "    model_3_10_epochs, x_test_np, y_test_np, model_format=\"keras\"\n",
    ")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(\n",
    "    model_3_10_epochs,\n",
    "    x_test_np,\n",
    "    y_test_np,\n",
    "    model_format=\"keras\",\n",
    "    show_prediction_evaluation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../../models/time_series_models_from_notebooks/cnn/hpo/cnn_time_ser_3_conv_layer_model.keras\n",
      "File size: 0.149 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"../../models/time_series_models_from_notebooks/cnn/hpo/cnn_time_ser_3_conv_layer_model.keras\"\n",
    "# model_3_10_epochs.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 layers model trained for 10 epochs will be used as final one as it has very good f1-score and really small size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../../models/time_series_models_from_notebooks/cnn/hpo/cnn_time_ser_3_conv_layer_model.keras\n"
     ]
    }
   ],
   "source": [
    "print(\"Model file name: \", MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
