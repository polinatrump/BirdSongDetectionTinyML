{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:29:34.722858: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-28 21:29:34.771460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 21:29:34.771487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 21:29:34.772559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 21:29:34.779394: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-28 21:29:34.780116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 21:29:35.513807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "import larq as lq\n",
    "import larq_compute_engine as lce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "import random\n",
    "tf.random.set_seed(3407)\n",
    "np.random.seed(3407)\n",
    "random.seed(3407)\n",
    "\n",
    "from create_spectrogram import (\n",
    "    create_spectrograms_from_audio_dataset, \n",
    ")\n",
    "from helper_functions import (\n",
    "    evaluate_prediction,\n",
    "    get_file_size, \n",
    "    convert_bytes, \n",
    "    convert_prefetchdataset_to_numpy_arrays,\n",
    "    predict_and_print_full_results,\n",
    "    evaluate_time_of_prediction,\n",
    "    full_int_model_predict,\n",
    "    get_f1_scores_of_non_overlapping_partitions_full_int_q,\n",
    "    get_f1_scores_of_bootstarping_partitions_full_int_q,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11292 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:29:37.903649: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-28 21:29:37.904065: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-28 21:29:37.972875: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2024-07-28 21:29:37.976914: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1393 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "Found 1380 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "Classes:  ['non_target' 'target']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.audio_dataset_from_directory(\"../dataset/training\", labels='inferred', sampling_rate=16000, batch_size=32, shuffle=True, seed=3407)\n",
    "test_dataset = tf.keras.utils.audio_dataset_from_directory(\"../dataset/testing\", labels='inferred', sampling_rate=16000, batch_size=32, shuffle=True, seed=3407)\n",
    "val_dataset = tf.keras.utils.audio_dataset_from_directory(\"../dataset/validation\", labels='inferred', sampling_rate=16000, batch_size=32, shuffle=True, seed=3407)\n",
    "\n",
    "label_names = np.array(train_dataset.class_names)\n",
    "print(\"Classes: \", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds = create_spectrograms_from_audio_dataset(train_dataset, sample_rate = 16000).cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_spectrogram_ds = create_spectrograms_from_audio_dataset(test_dataset, sample_rate = 16000).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_spectrogram_ds = create_spectrograms_from_audio_dataset(val_dataset, sample_rate = 16000).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "x_train_np, y_train_np = convert_prefetchdataset_to_numpy_arrays(train_spectrogram_ds)\n",
    "x_val_np, y_val_np = convert_prefetchdataset_to_numpy_arrays(val_spectrogram_ds)\n",
    "x_test_np, y_test_np = convert_prefetchdataset_to_numpy_arrays(test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0], shape=(32,), dtype=int32)\n",
      "0\n",
      "tf.Tensor([0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n",
      "0\n",
      "tf.Tensor([1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0], shape=(32,), dtype=int32)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# non-target = 0, target = 1\n",
    "# Print spectrograms of target or non-target class from dataset\n",
    "class_number = 1\n",
    "\n",
    "for i in train_spectrogram_ds:\n",
    "    print(i[1])\n",
    "    print(i[1][0].numpy())\n",
    "    if i[1][0].numpy() == class_number:\n",
    "        spectrogram = i[0][0]\n",
    "        height = spectrogram.shape[0]\n",
    "        width = spectrogram.shape[1]\n",
    "        X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "        Y = range(height)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.pcolormesh(X, Y, spectrogram)\n",
    "        plt.title(i[1][0])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Frequency (Hz)')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes, num_of_layers):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.num_of_conv_layers = num_of_layers\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        kernel_size = hp.Choice('kernel_size', values=[2, 3, 5])\n",
    "        dense_units = hp.Int('1st_dense_units', min_value=4, max_value=32, step=4)\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        dense_activation = hp.Choice('2nd_dense_activation', values=['softmax', 'sigmoid'])\n",
    "\n",
    "        # Model architecture\n",
    "        # Tune number of layers\n",
    "        for i in range(self.num_of_conv_layers):\n",
    "            if i == 0:\n",
    "                model.add(lq.layers.QuantConv2D(\n",
    "                    filters=hp.Int(f'filters_{i}', min_value=2, max_value=8, step=2), \n",
    "                    kernel_size=(kernel_size, kernel_size),\n",
    "                    input_shape=self.input_shape,\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "            else:\n",
    "                model.add(lq.layers.QuantConv2D(\n",
    "                    filters=hp.Int(f'filters_{i}', min_value=2, max_value=8, step=2), \n",
    "                    kernel_size=(kernel_size, kernel_size),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "            model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "            model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        model.add(lq.layers.QuantDense(\n",
    "            units=dense_units,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False\n",
    "        ))\n",
    "        model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "        model.add(lq.layers.QuantDense(\n",
    "            units=self.num_classes,\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False\n",
    "        ))\n",
    "        model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "        model.add(tf.keras.layers.Activation(dense_activation))\n",
    "       \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 1 conv2d layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 50s]\n",
      "val_accuracy: 0.9315217435359955\n",
      "\n",
      "Best val_accuracy So Far: 0.9554347693920135\n",
      "Total elapsed time: 00h 37m 35s\n",
      "Results summary\n",
      "Results in ../hpo_tuner/bnn/bnn_1_conv2d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.001585943564699532\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "Score: 0.9554347693920135\n",
      "\n",
      "Trial 11 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 32\n",
      "learning_rate: 0.0023095567943677625\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 6\n",
      "Score: 0.9554347693920135\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 4\n",
      "learning_rate: 0.00618012909224231\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 4\n",
      "Score: 0.9514492750167847\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0010705522855084585\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 8\n",
      "Score: 0.949999988079071\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 8\n",
      "learning_rate: 0.00502327522629524\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "Score: 0.9427536129951477\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.006542820504404195\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 8\n",
      "Score: 0.9405797123908997\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 16\n",
      "learning_rate: 0.0030388660376328537\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "Score: 0.9326086938381195\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 16\n",
      "learning_rate: 0.0016941070795721022\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 6\n",
      "Score: 0.9315217435359955\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0011317251173931062\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "Score: 0.9297101497650146\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 16\n",
      "learning_rate: 0.007086295335801266\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "Score: 0.9289855062961578\n"
     ]
    }
   ],
   "source": [
    "input_shape = (184, 80, 1)\n",
    "num_classes = 2\n",
    "\n",
    "num_of_conv_layers = 1\n",
    "tuner_1_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_conv_layers),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='../hpo_tuner/bnn',\n",
    "    project_name='bnn_mel_spec_1_conv2d_tuning'\n",
    ")\n",
    "tuner_1_layer_cnn.search(train_spectrogram_ds, epochs=5, validation_data=val_spectrogram_ds)\n",
    "tuner_1_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 2 conv2d layers CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 25s]\n",
      "val_accuracy: 0.8949275612831116\n",
      "\n",
      "Best val_accuracy So Far: 0.9398550689220428\n",
      "Total elapsed time: 00h 53m 06s\n",
      "Results summary\n",
      "Results in ../hpo_tuner/bnn/bnn_2_conv2d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0063881645876014866\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 4\n",
      "filters_1: 4\n",
      "Score: 0.9398550689220428\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 8\n",
      "learning_rate: 0.007515970819203388\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 8\n",
      "filters_1: 4\n",
      "Score: 0.9376811683177948\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 32\n",
      "learning_rate: 0.0037652861684021495\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "filters_1: 8\n",
      "Score: 0.9329710006713867\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0013810908036985007\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "filters_1: 2\n",
      "Score: 0.9322463870048523\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.00041615784427578373\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 8\n",
      "filters_1: 8\n",
      "Score: 0.9300724565982819\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 16\n",
      "learning_rate: 0.000331710264204527\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "filters_1: 4\n",
      "Score: 0.9264492690563202\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 4\n",
      "learning_rate: 0.009335569435817186\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 4\n",
      "filters_1: 4\n",
      "Score: 0.9246376752853394\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 32\n",
      "learning_rate: 0.00022929216604337032\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 8\n",
      "filters_1: 4\n",
      "Score: 0.9202898442745209\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.00536658240702523\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 2\n",
      "filters_1: 4\n",
      "Score: 0.9130434691905975\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 32\n",
      "learning_rate: 0.0001522298574808984\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "filters_1: 8\n",
      "Score: 0.9050724804401398\n"
     ]
    }
   ],
   "source": [
    "num_of_layers = 2\n",
    "tuner_2_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_layers),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='../hpo_tuner/bnn',\n",
    "    project_name='bnn_mel_spec_2_conv2d_tuning'\n",
    ")\n",
    "tuner_2_layer_cnn.search(train_spectrogram_ds, epochs=5, validation_data=val_spectrogram_ds)\n",
    "tuner_2_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune 3 conv2d layers CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 29s]\n",
      "val_accuracy: 0.9268116056919098\n",
      "\n",
      "Best val_accuracy So Far: 0.9485507309436798\n",
      "Total elapsed time: 00h 54m 30s\n",
      "Results summary\n",
      "Results in ../hpo_tuner/bnn/bnn_3_conv2d_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.0019990629503147854\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 6\n",
      "filters_1: 4\n",
      "filters_2: 8\n",
      "Score: 0.9485507309436798\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.0030956184222862408\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 2\n",
      "filters_1: 6\n",
      "filters_2: 4\n",
      "Score: 0.9286231994628906\n",
      "\n",
      "Trial 14 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 32\n",
      "learning_rate: 0.003499257561375248\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "filters_1: 2\n",
      "filters_2: 4\n",
      "Score: 0.9286231696605682\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0007860854030984577\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 6\n",
      "filters_1: 8\n",
      "filters_2: 4\n",
      "Score: 0.9268116056919098\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.004122693745389103\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "filters_1: 4\n",
      "filters_2: 8\n",
      "Score: 0.9217391312122345\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.00912470672476799\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n",
      "filters_1: 8\n",
      "filters_2: 4\n",
      "Score: 0.9195652306079865\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0031378187626006464\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 2\n",
      "filters_1: 4\n",
      "filters_2: 8\n",
      "Score: 0.9119565188884735\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0007703289445934244\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 6\n",
      "filters_1: 6\n",
      "filters_2: 2\n",
      "Score: 0.8989130556583405\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 12\n",
      "learning_rate: 0.001601348848498851\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 2\n",
      "filters_1: 2\n",
      "filters_2: 4\n",
      "Score: 0.8974637687206268\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "kernel_size: 3\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0005572109949030198\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 2\n",
      "filters_1: 6\n",
      "filters_2: 4\n",
      "Score: 0.8873188197612762\n"
     ]
    }
   ],
   "source": [
    "num_of_layers = 3\n",
    "tuner_3_layer_cnn = RandomSearch(\n",
    "    CNNHyperModel(input_shape, num_classes, num_of_layers),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='../hpo_tuner/bnn',\n",
    "    project_name='bnn_mel_spec_3_conv2d_tuning'\n",
    ")\n",
    "tuner_3_layer_cnn.search(train_spectrogram_ds, epochs=5, validation_data=val_spectrogram_ds)\n",
    "tuner_3_layer_cnn.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "tuner_1_layer_cnn.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "filters_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner_2_layer_cnn.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "kernel_size (Choice)\n",
      "{'default': 2, 'conditions': [], 'values': [2, 3, 5], 'ordered': True}\n",
      "1st_dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 32, 'step': 4, 'sampling': 'linear'}\n",
      "2nd_dense_activation (Choice)\n",
      "{'default': 'softmax', 'conditions': [], 'values': ['softmax', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "filters_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "conv2d_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "filters_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n",
      "filters_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "tuner_3_layer_cnn.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "2nd_dense_activation: softmax\n",
      "learning_rate: 0.0002630359449493652\n",
      "filters_0: 6\n",
      "conv2d_activation: relu\n",
      "filters_1: 6\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_2_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ... models, that showed good (or best) results in HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.001585943564699532\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 8\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_1_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d_3 (QuantConv2  (None, 180, 76, 8)        200       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 90, 38, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 90, 38, 8)         24        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 27360)             0         \n",
      "                                                                 \n",
      " quant_dense_6 (QuantDense)  (None, 28)                766080    \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 28)                84        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_7 (QuantDense)  (None, 2)                 56        \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 2)                 6         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 766450 (2.92 MB)\n",
      "Trainable params: 766374 (2.92 MB)\n",
      "Non-trainable params: 76 (304.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_20_epochs = tf.keras.models.Sequential()\n",
    "\n",
    "# The first layer, only the weights are quantized while activations are left full-precision\n",
    "model_1_20_epochs.add(lq.layers.QuantConv2D(filters=8, \n",
    "                                  kernel_size=(5, 5),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_1_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_1_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_1_20_epochs.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_1_20_epochs.add(lq.layers.QuantDense(units=28, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_1_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_1_20_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_1_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_1_20_epochs.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model_1_20_epochs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 15s 40ms/step - loss: 0.3995 - accuracy: 0.8344\n",
      "Epoch 2/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.2859 - accuracy: 0.8972\n",
      "Epoch 3/20\n",
      "353/353 [==============================] - 13s 38ms/step - loss: 0.2509 - accuracy: 0.9163\n",
      "Epoch 4/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.2139 - accuracy: 0.9384\n",
      "Epoch 5/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1933 - accuracy: 0.9528\n",
      "Epoch 6/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1943 - accuracy: 0.9532\n",
      "Epoch 7/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1801 - accuracy: 0.9617\n",
      "Epoch 8/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1801 - accuracy: 0.9632\n",
      "Epoch 9/20\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1748 - accuracy: 0.9651\n",
      "Epoch 10/20\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1681 - accuracy: 0.9677\n",
      "Epoch 11/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1607 - accuracy: 0.9756\n",
      "Epoch 12/20\n",
      "353/353 [==============================] - 14s 41ms/step - loss: 0.1614 - accuracy: 0.9770\n",
      "Epoch 13/20\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1525 - accuracy: 0.9793\n",
      "Epoch 14/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1516 - accuracy: 0.9806\n",
      "Epoch 15/20\n",
      "353/353 [==============================] - 15s 42ms/step - loss: 0.1571 - accuracy: 0.9779\n",
      "Epoch 16/20\n",
      "353/353 [==============================] - 15s 43ms/step - loss: 0.1522 - accuracy: 0.9796\n",
      "Epoch 17/20\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1510 - accuracy: 0.9826\n",
      "Epoch 18/20\n",
      "353/353 [==============================] - 16s 45ms/step - loss: 0.1442 - accuracy: 0.9868\n",
      "Epoch 19/20\n",
      "353/353 [==============================] - 14s 41ms/step - loss: 0.1463 - accuracy: 0.9849\n",
      "Epoch 20/20\n",
      "353/353 [==============================] - 14s 39ms/step - loss: 0.1442 - accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "model_1_20_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001585943564699532),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 20\n",
    "history = model_1_20_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 11ms/step - loss: 0.2070 - accuracy: 0.9514\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_1_20_epochs.evaluate(x_val_np, y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 11ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.14%\n",
      "Recall: 89.62%\n",
      "Precision: 95.92%\n",
      "F1-score: 92.66%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9241776217225649\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.931604512015737\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 0s 10ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.76%\n",
      "Recall: 90.83%\n",
      "Precision: 96.07%\n",
      "F1-score: 93.38%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9346979852835252\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9332818501990914\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 95.76%\n",
      "Recall: 90.83%\n",
      "Precision: 96.07%\n",
      "F1-score: 93.38%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.049 seconds\n",
      "Max: 0.119 seconds\n",
      "Min: 0.043 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_1_20_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_1_20_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_1_20_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_1_conv_layer_model.keras\n",
      "File size: 8.822 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_1_conv_layer_model.keras\"\n",
    "model_1_20_epochs.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d (QuantConv2D)  (None, 180, 76, 8)        200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 90, 38, 8)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 90, 38, 8)         24        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 27360)             0         \n",
      "                                                                 \n",
      " quant_dense (QuantDense)    (None, 28)                766080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 28)                84        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " quant_dense_1 (QuantDense)  (None, 2)                 56        \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 2)                 6         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 766450 (2.92 MB)\n",
      "Trainable params: 766374 (2.92 MB)\n",
      "Non-trainable params: 76 (304.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 16s 43ms/step - loss: 0.4209 - accuracy: 0.8210\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 14s 41ms/step - loss: 0.2911 - accuracy: 0.8943\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.2397 - accuracy: 0.9237\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.2129 - accuracy: 0.9412\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1902 - accuracy: 0.9538\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 16s 44ms/step - loss: 0.1821 - accuracy: 0.9600\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 16s 44ms/step - loss: 0.1780 - accuracy: 0.9631\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 15s 41ms/step - loss: 0.1726 - accuracy: 0.9666\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1727 - accuracy: 0.9662\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 14s 40ms/step - loss: 0.1648 - accuracy: 0.9722\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 11ms/step - loss: 0.1847 - accuracy: 0.9565\n",
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 11ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.65%\n",
      "Recall: 95.76%\n",
      "Precision: 91.87%\n",
      "F1-score: 93.78%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9363134992727249\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9392887725851077\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 12ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.05%\n",
      "Recall: 95.41%\n",
      "Precision: 90.10%\n",
      "F1-score: 92.68%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9280263182234367\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9240014758441233\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 95.05%\n",
      "Recall: 95.41%\n",
      "Precision: 90.10%\n",
      "F1-score: 92.68%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.05 seconds\n",
      "Max: 0.199 seconds\n",
      "Min: 0.044 seconds\n"
     ]
    }
   ],
   "source": [
    "model_1_10_epochs = tf.keras.models.Sequential()\n",
    "model_1_10_epochs.add(lq.layers.QuantConv2D(filters=8, \n",
    "                                  kernel_size=(5, 5),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_1_10_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_1_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_1_10_epochs.add(tf.keras.layers.Flatten())\n",
    "model_1_10_epochs.add(lq.layers.QuantDense(units=28, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_1_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_1_10_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_1_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_1_10_epochs.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model_1_10_epochs.summary()\n",
    "\n",
    "model_1_10_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001585943564699532),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 10\n",
    "history = model_1_10_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_1_10_epochs.evaluate(x_val_np, y_val_np)\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_1_10_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_1_10_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_1_10_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer CNN is too big. We dont have second MaxPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 5\n",
      "1st_dense_units: 28\n",
      "learning_rate: 0.0063881645876014866\n",
      "2nd_dense_activation: softmax\n",
      "filters_0: 4\n",
      "filters_1: 4\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_2_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d_12 (QuantConv  (None, 180, 76, 4)        100       \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 90, 38, 4)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 90, 38, 4)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_conv2d_13 (QuantConv  (None, 86, 34, 4)         400       \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 43, 17, 4)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 43, 17, 4)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2924)              0         \n",
      "                                                                 \n",
      " quant_dense_12 (QuantDense  (None, 28)                81872     \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 28)                84        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_13 (QuantDense  (None, 2)                 56        \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 2)                 6         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82542 (322.43 KB)\n",
      "Trainable params: 82466 (322.13 KB)\n",
      "Non-trainable params: 76 (304.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 19s 50ms/step - loss: 0.4088 - accuracy: 0.8237\n",
      "Epoch 2/20\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.2720 - accuracy: 0.9007\n",
      "Epoch 3/20\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.2514 - accuracy: 0.9120\n",
      "Epoch 4/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2443 - accuracy: 0.9154\n",
      "Epoch 5/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2331 - accuracy: 0.9205\n",
      "Epoch 6/20\n",
      "353/353 [==============================] - 16s 47ms/step - loss: 0.2323 - accuracy: 0.9191\n",
      "Epoch 7/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2278 - accuracy: 0.9228\n",
      "Epoch 8/20\n",
      "353/353 [==============================] - 17s 48ms/step - loss: 0.2181 - accuracy: 0.9302\n",
      "Epoch 9/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2262 - accuracy: 0.9243\n",
      "Epoch 10/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2134 - accuracy: 0.9343\n",
      "Epoch 11/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2112 - accuracy: 0.9397\n",
      "Epoch 12/20\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2081 - accuracy: 0.9410\n",
      "Epoch 13/20\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.2087 - accuracy: 0.9405\n",
      "Epoch 14/20\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.2062 - accuracy: 0.9424\n",
      "Epoch 15/20\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.2030 - accuracy: 0.9447\n",
      "Epoch 16/20\n",
      "353/353 [==============================] - 18s 51ms/step - loss: 0.2038 - accuracy: 0.9463\n",
      "Epoch 17/20\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.2032 - accuracy: 0.9442\n",
      "Epoch 18/20\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.1951 - accuracy: 0.9492\n",
      "Epoch 19/20\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.1918 - accuracy: 0.9537\n",
      "Epoch 20/20\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.1915 - accuracy: 0.9553\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.2141 - accuracy: 0.9370\n",
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 13ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 93.70%\n",
      "Recall: 91.31%\n",
      "Precision: 90.36%\n",
      "F1-score: 90.83%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9076405943526753\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9122990467710346\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 13ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 93.83%\n",
      "Recall: 89.08%\n",
      "Precision: 91.89%\n",
      "F1-score: 90.47%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9048857134038025\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9079555492010022\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 93.83%\n",
      "Recall: 89.08%\n",
      "Precision: 91.89%\n",
      "F1-score: 90.47%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.052 seconds\n",
      "Max: 0.145 seconds\n",
      "Min: 0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "model_2_20_epochs = tf.keras.models.Sequential()\n",
    "\n",
    "# The first layer, only the weights are quantized while activations are left full-precision\n",
    "model_2_20_epochs.add(lq.layers.QuantConv2D(filters=4, \n",
    "                                  kernel_size=(5, 5),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_2_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_2_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_20_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=4, \n",
    "                    kernel_size=(5, 5),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_2_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_2_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_20_epochs.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_2_20_epochs.add(lq.layers.QuantDense(units=28, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_2_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_20_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_2_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_2_20_epochs.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model_2_20_epochs.summary()\n",
    "\n",
    "\n",
    "model_2_20_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0063881645876014866),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 20\n",
    "history = model_2_20_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_2_20_epochs.evaluate(x_val_np, y_val_np)\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_2_20_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_2_20_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_2_20_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_2_conv_layer_model.keras\n",
      "File size: 1.009 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_2_conv_layer_model.keras\"\n",
    "model_2_20_epochs.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d_8 (QuantConv2  (None, 180, 76, 4)        100       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 90, 38, 4)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 90, 38, 4)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_conv2d_9 (QuantConv2  (None, 86, 34, 4)         400       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 43, 17, 4)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 43, 17, 4)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2924)              0         \n",
      "                                                                 \n",
      " quant_dense_8 (QuantDense)  (None, 28)                81872     \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 28)                84        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_9 (QuantDense)  (None, 2)                 56        \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 2)                 6         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82542 (322.43 KB)\n",
      "Trainable params: 82466 (322.13 KB)\n",
      "Non-trainable params: 76 (304.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 18s 49ms/step - loss: 0.3924 - accuracy: 0.8350\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 17s 48ms/step - loss: 0.2628 - accuracy: 0.9054\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 16s 47ms/step - loss: 0.2339 - accuracy: 0.9237\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2198 - accuracy: 0.9312\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 16s 47ms/step - loss: 0.2087 - accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 17s 48ms/step - loss: 0.2084 - accuracy: 0.9392\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2029 - accuracy: 0.9437\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.2124 - accuracy: 0.9373\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 17s 49ms/step - loss: 0.2061 - accuracy: 0.9416\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 17s 48ms/step - loss: 0.1991 - accuracy: 0.9476\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3003 - accuracy: 0.8362\n",
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 14ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 83.62%\n",
      "Recall: 52.97%\n",
      "Precision: 98.43%\n",
      "F1-score: 68.87%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.6777018901494355\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.6849706597532347\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 13ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 86.36%\n",
      "Recall: 58.73%\n",
      "Precision: 99.63%\n",
      "F1-score: 73.90%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.7393501675475327\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.7374487550303647\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 86.36%\n",
      "Recall: 58.73%\n",
      "Precision: 99.63%\n",
      "F1-score: 73.90%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.051 seconds\n",
      "Max: 0.132 seconds\n",
      "Min: 0.045 seconds\n"
     ]
    }
   ],
   "source": [
    "model_2_10_epochs = tf.keras.models.Sequential()\n",
    "\n",
    "# The first layer, only the weights are quantized while activations are left full-precision\n",
    "model_2_10_epochs.add(lq.layers.QuantConv2D(filters=4, \n",
    "                                  kernel_size=(5, 5),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_2_10_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_2_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_10_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=4, \n",
    "                    kernel_size=(5, 5),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_2_10_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_2_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_10_epochs.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_2_10_epochs.add(lq.layers.QuantDense(units=28, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_2_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_2_10_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_2_10_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_2_10_epochs.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model_2_10_epochs.summary()\n",
    "\n",
    "\n",
    "model_2_10_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0063881645876014866),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 10\n",
    "history = model_2_10_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_2_10_epochs.evaluate(x_val_np, y_val_np)\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_2_10_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_2_10_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_2_10_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "kernel_size: 2\n",
      "1st_dense_units: 24\n",
      "learning_rate: 0.0019990629503147854\n",
      "2nd_dense_activation: sigmoid\n",
      "filters_0: 6\n",
      "filters_1: 4\n",
      "filters_2: 8\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner_3_layer_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d_3 (QuantConv2  (None, 183, 79, 6)        24        \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 91, 39, 6)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 91, 39, 6)         18        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " quant_conv2d_4 (QuantConv2  (None, 90, 38, 4)         96        \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 45, 19, 4)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 45, 19, 4)         12        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " quant_conv2d_5 (QuantConv2  (None, 44, 18, 8)         128       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 22, 9, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 22, 9, 8)          24        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1584)              0         \n",
      "                                                                 \n",
      " quant_dense_2 (QuantDense)  (None, 24)                38016     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 24)                72        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " quant_dense_3 (QuantDense)  (None, 2)                 48        \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 2)                 6         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38444 (150.17 KB)\n",
      "Trainable params: 38356 (149.83 KB)\n",
      "Non-trainable params: 88 (352.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "353/353 [==============================] - 18s 48ms/step - loss: 0.3706 - accuracy: 0.8468\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 16s 46ms/step - loss: 0.2797 - accuracy: 0.8966\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 17s 48ms/step - loss: 0.2607 - accuracy: 0.9057\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2739 - accuracy: 0.8967\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 18s 51ms/step - loss: 0.2508 - accuracy: 0.9117\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 16s 46ms/step - loss: 0.2251 - accuracy: 0.9303\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 19s 54ms/step - loss: 0.2250 - accuracy: 0.9242\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 18s 50ms/step - loss: 0.2161 - accuracy: 0.9335\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2074 - accuracy: 0.9401\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 16s 46ms/step - loss: 0.2062 - accuracy: 0.9404\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1805 - accuracy: 0.9594\n",
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 13ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 95.94%\n",
      "Recall: 90.68%\n",
      "Precision: 97.27%\n",
      "F1-score: 93.86%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9374112727516213\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9397700049149073\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 17ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 94.97%\n",
      "Recall: 87.12%\n",
      "Precision: 97.32%\n",
      "F1-score: 91.94%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9197318103358368\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9138569856995001\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 94.97%\n",
      "Recall: 87.12%\n",
      "Precision: 97.32%\n",
      "F1-score: 91.94%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.069 seconds\n",
      "Max: 0.244 seconds\n",
      "Min: 0.052 seconds\n"
     ]
    }
   ],
   "source": [
    "model_3_20_epochs = tf.keras.models.Sequential()\n",
    "\n",
    "# The first layer, only the weights are quantized while activations are left full-precision\n",
    "model_3_20_epochs.add(lq.layers.QuantConv2D(filters=6, \n",
    "                                  kernel_size=(2, 2),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_3_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_20_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=4, \n",
    "                    kernel_size=(2, 2),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_3_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_20_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=8, \n",
    "                    kernel_size=(2, 2),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_3_20_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_20_epochs.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_3_20_epochs.add(lq.layers.QuantDense(units=24, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_3_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_20_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_3_20_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_3_20_epochs.add(tf.keras.layers.Activation(\"sigmoid\"))\n",
    "\n",
    "model_3_20_epochs.summary()\n",
    "\n",
    "\n",
    "model_3_20_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0019990629503147854),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 10\n",
    "history = model_3_20_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_3_20_epochs.evaluate(x_val_np, y_val_np)\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_3_20_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_3_20_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_3_20_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quant_conv2d_6 (QuantConv2  (None, 183, 79, 6)        24        \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 91, 39, 6)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 91, 39, 6)         18        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_conv2d_7 (QuantConv2  (None, 90, 38, 4)         96        \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 45, 19, 4)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 45, 19, 4)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_conv2d_8 (QuantConv2  (None, 44, 18, 8)         128       \n",
      " D)                                                              \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 22, 9, 8)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 22, 9, 8)          24        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1584)              0         \n",
      "                                                                 \n",
      " quant_dense_4 (QuantDense)  (None, 24)                38016     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 24)                72        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_5 (QuantDense)  (None, 2)                 48        \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 2)                 6         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38444 (150.17 KB)\n",
      "Trainable params: 38356 (149.83 KB)\n",
      "Non-trainable params: 88 (352.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/newname/.venv/lib/python3.9/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 19s 48ms/step - loss: 0.4202 - accuracy: 0.8204\n",
      "Epoch 2/5\n",
      "353/353 [==============================] - 18s 51ms/step - loss: 0.3154 - accuracy: 0.8752\n",
      "Epoch 3/5\n",
      "353/353 [==============================] - 18s 52ms/step - loss: 0.2624 - accuracy: 0.9083\n",
      "Epoch 4/5\n",
      "353/353 [==============================] - 17s 47ms/step - loss: 0.2295 - accuracy: 0.9261\n",
      "Epoch 5/5\n",
      "353/353 [==============================] - 17s 50ms/step - loss: 0.2326 - accuracy: 0.9293\n",
      "Validation dataset accuracy:\n",
      "44/44 [==============================] - 1s 17ms/step - loss: 0.2107 - accuracy: 0.9420\n",
      "Validation dataset:\n",
      "44/44 [==============================] - 1s 15ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 94.20%\n",
      "Recall: 84.11%\n",
      "Precision: 98.76%\n",
      "F1-score: 90.85%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9074523758568377\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9050684415677962\n",
      "\n",
      "Test dataset:\n",
      "44/44 [==============================] - 1s 16ms/step\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 93.75%\n",
      "Recall: 84.28%\n",
      "Precision: 96.26%\n",
      "F1-score: 89.87%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.8998623711485187\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.8956799222631681\n",
      "Time of one prediction for Test dataset:\n",
      "Accuracy: 93.75%\n",
      "Recall: 84.28%\n",
      "Precision: 96.26%\n",
      "F1-score: 89.87%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.069 seconds\n",
      "Max: 0.215 seconds\n",
      "Min: 0.049 seconds\n"
     ]
    }
   ],
   "source": [
    "model_3_5_epochs = tf.keras.models.Sequential()\n",
    "\n",
    "# The first layer, only the weights are quantized while activations are left full-precision\n",
    "model_3_5_epochs.add(lq.layers.QuantConv2D(filters=6, \n",
    "                                  kernel_size=(2, 2),\n",
    "                                  kernel_quantizer=\"ste_sign\",\n",
    "                                  kernel_constraint=\"weight_clip\",\n",
    "                                  use_bias=False,\n",
    "                                  input_shape=(184, 80, 1)))\n",
    "model_3_5_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_5_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_5_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=4, \n",
    "                    kernel_size=(2, 2),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_3_5_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_5_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_5_epochs.add(lq.layers.QuantConv2D(\n",
    "                    filters=8, \n",
    "                    kernel_size=(2, 2),\n",
    "                    input_quantizer=\"ste_sign\",\n",
    "                    kernel_quantizer=\"ste_sign\",\n",
    "                    kernel_constraint=\"weight_clip\",\n",
    "                    use_bias=False\n",
    "                ))\n",
    "model_3_5_epochs.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model_3_5_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_5_epochs.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model_3_5_epochs.add(lq.layers.QuantDense(units=24, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_3_5_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_3_5_epochs.add(lq.layers.QuantDense(units=2, \n",
    "                                 use_bias=False, \n",
    "                                 input_quantizer=\"ste_sign\",\n",
    "                                 kernel_quantizer=\"ste_sign\",\n",
    "                                 kernel_constraint=\"weight_clip\"))\n",
    "model_3_5_epochs.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model_3_5_epochs.add(tf.keras.layers.Activation(\"sigmoid\"))\n",
    "\n",
    "model_3_5_epochs.summary()\n",
    "\n",
    "\n",
    "model_3_5_epochs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0019990629503147854),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# EPOCHS = 1\n",
    "EPOCHS = 5\n",
    "history = model_3_5_epochs.fit(\n",
    "    train_spectrogram_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Validation dataset accuracy:\")\n",
    "val_loss, val_acc = model_3_5_epochs.evaluate(x_val_np, y_val_np)\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "(\n",
    "    y_pred_val, \n",
    "    non_overlap_patritions_f1_scores_val, \n",
    "    bootstrap_patritions_f1_scores_val,\n",
    ") = predict_and_print_full_results(model_3_5_epochs, x_val_np, y_val_np, model_format=\"keras\")\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    ") = predict_and_print_full_results(model_3_5_epochs, x_test_np, y_test_np, model_format=\"keras\")\n",
    "\n",
    "print(\"Time of one prediction for Test dataset:\")\n",
    "evaluate_time_of_prediction(model_3_5_epochs, x_test_np, y_test_np, model_format=\"keras\", show_prediction_evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model.keras\n",
      "File size: 0.519 Megabytes\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model.keras\"\n",
    "model_3_20_epochs.save(MODEL_FILE_NAME)\n",
    "print(\"Model file name: \", MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(MODEL_FILE_NAME), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_20_epochs = tf.keras.models.load_model(\"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model has good f1-score and was the smallest (519 Kb)\n",
    "\n",
    "I will try to make it smaller and keep this level of f1-score\n",
    "\n",
    "It was hard to make it smaller without loss in f1-score so I decided to use this model as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/44 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 21:33:20.875258: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 82019840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = model_3_20_epochs.predict(x_test_np)\n",
    "y_pred = tf.argmax(y_pred_prob, axis=1).numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show latent weights of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: quant_conv2d_3\n",
      "Shape: (2, 2, 1, 6)\n",
      "Unique values: [-0.58556724 -0.4760366  -0.42140493 -0.33268762 -0.3153949  -0.10318995\n",
      " -0.08034132 -0.07063703 -0.0541437   0.00247525  0.02785805  0.02840374\n",
      "  0.09357753  0.12507509  0.12645264  0.16857198  0.18586649  0.1963162\n",
      "  0.37236747  0.4506057   0.4587317   0.6247901   0.65256023  0.93908894]\n",
      "\n",
      "First few weights: [-0.07063703  0.12507509  0.4506057   0.00247525 -0.33268762  0.02840374\n",
      "  0.93908894 -0.3153949   0.02785805  0.4587317 ]\n",
      "\n",
      "Layer: quant_conv2d_4\n",
      "Shape: (2, 2, 6, 4)\n",
      "Unique values: [-8.1961292e-01 -6.6805083e-01 -5.6972295e-01 -5.5980152e-01\n",
      " -5.2410978e-01 -5.0751966e-01 -5.0569969e-01 -4.5249176e-01\n",
      " -4.0741798e-01 -3.9473873e-01 -3.9230612e-01 -3.7853619e-01\n",
      " -3.5750267e-01 -3.3168477e-01 -3.2919437e-01 -3.2322365e-01\n",
      " -3.1101790e-01 -2.9838705e-01 -2.7472708e-01 -2.6714271e-01\n",
      " -2.1595439e-01 -2.0875093e-01 -1.8329845e-01 -1.6528052e-01\n",
      " -1.1749125e-01 -1.1125868e-01 -9.5753163e-02 -8.2675077e-02\n",
      " -7.7510871e-02 -6.9486335e-02 -4.8846729e-02 -3.3317454e-02\n",
      " -3.0765299e-02 -3.0731391e-02 -2.5495527e-02 -2.0132234e-02\n",
      " -1.3434852e-02 -1.2379314e-02 -7.6035741e-03 -2.1116261e-03\n",
      " -1.4678717e-03  3.4713044e-04  5.9323740e-04  3.2189093e-03\n",
      "  5.9082317e-03  8.4980316e-03  1.4397042e-02  2.9672047e-02\n",
      "  3.7269417e-02  3.8092647e-02  4.0847678e-02  5.3430263e-02\n",
      "  7.0249878e-02  8.6214200e-02  8.7210149e-02  9.7883679e-02\n",
      "  1.2908950e-01  1.4326578e-01  1.4437254e-01  1.5223742e-01\n",
      "  1.6151640e-01  1.8081577e-01  1.8362467e-01  1.8745135e-01\n",
      "  2.0674779e-01  2.1438327e-01  2.1536398e-01  2.1644096e-01\n",
      "  2.3279171e-01  2.3995052e-01  2.5476837e-01  2.5636956e-01\n",
      "  2.6996818e-01  2.7806726e-01  2.8247458e-01  3.0825683e-01\n",
      "  3.1907934e-01  3.4235436e-01  3.5103476e-01  3.5785577e-01\n",
      "  4.2567360e-01  4.7064817e-01  5.0213271e-01  5.0975883e-01\n",
      "  5.5193681e-01  5.5231446e-01  6.1367959e-01  6.3518494e-01\n",
      "  6.9995606e-01  7.3406607e-01  7.6592070e-01  8.1284392e-01\n",
      "  9.0601259e-01  1.0000000e+00]\n",
      "\n",
      "First few weights: [ 0.09788368 -0.3785362   0.25636956 -0.45249176 -0.2671427   0.02967205\n",
      " -0.01237931 -0.07751087 -0.18329845  0.03809265]\n",
      "\n",
      "Layer: quant_conv2d_5\n",
      "Shape: (2, 2, 4, 8)\n",
      "Unique values: [-1.00000000e+00 -9.96920347e-01 -9.74100292e-01 -9.44008172e-01\n",
      " -8.92601490e-01 -8.86865795e-01 -8.78188431e-01 -7.70685911e-01\n",
      " -6.32828832e-01 -6.25605941e-01 -6.03549123e-01 -5.36036074e-01\n",
      " -5.01961708e-01 -4.99622911e-01 -4.59786713e-01 -4.31483507e-01\n",
      " -4.20513481e-01 -4.19339418e-01 -4.06459898e-01 -4.02540773e-01\n",
      " -3.98727864e-01 -3.43291819e-01 -3.38746548e-01 -3.07058126e-01\n",
      " -3.05803508e-01 -2.81348944e-01 -2.77865797e-01 -2.74599165e-01\n",
      " -2.68444121e-01 -2.63408899e-01 -2.62413293e-01 -2.36591280e-01\n",
      " -2.17092425e-01 -1.89801961e-01 -1.84268981e-01 -1.83878258e-01\n",
      " -1.60587594e-01 -1.51314601e-01 -1.46688879e-01 -1.30672619e-01\n",
      " -1.11819416e-01 -8.31886306e-02 -7.18596727e-02 -6.26675338e-02\n",
      " -6.23294190e-02 -5.31072989e-02 -4.74243164e-02 -3.72539274e-02\n",
      " -3.68829258e-02 -1.73169412e-02 -1.66148581e-02 -1.65255144e-02\n",
      " -1.16787115e-02 -1.14557892e-02 -5.54251205e-03 -3.45679070e-03\n",
      " -3.03492486e-03 -2.91861035e-03 -2.63873395e-03 -2.35247961e-03\n",
      " -1.44895411e-03 -1.08815904e-03 -9.64348961e-04 -6.15394907e-04\n",
      "  3.75368109e-05  5.78078325e-05  2.99662910e-03  3.03762755e-03\n",
      "  7.40729179e-03  9.93177202e-03  1.44947190e-02  1.50966896e-02\n",
      "  1.57202110e-02  2.55527310e-02  2.64207684e-02  4.10818867e-02\n",
      "  4.90572751e-02  5.71144074e-02  6.13408163e-02  6.26456514e-02\n",
      "  6.50896803e-02  7.22439289e-02  8.42319578e-02  8.97034705e-02\n",
      "  1.04682744e-01  1.19683988e-01  1.29427254e-01  1.29512757e-01\n",
      "  1.33542463e-01  1.43607095e-01  1.45512626e-01  1.45663574e-01\n",
      "  1.45790443e-01  1.90165848e-01  2.11418360e-01  2.17280105e-01\n",
      "  2.19802007e-01  2.32091054e-01  2.38482922e-01  2.47113615e-01\n",
      "  2.57564753e-01  2.82845795e-01  3.30866635e-01  3.58297825e-01\n",
      "  3.75529587e-01  3.91874105e-01  4.19304013e-01  4.22120988e-01\n",
      "  4.40614730e-01  4.74665344e-01  4.94580209e-01  5.24264574e-01\n",
      "  5.50463438e-01  5.54246664e-01  5.65031111e-01  5.68780601e-01\n",
      "  5.83628476e-01  5.89285016e-01  5.90670228e-01  6.08659327e-01\n",
      "  6.36182964e-01  6.63578749e-01  7.58315444e-01  7.61482060e-01\n",
      "  8.41762245e-01  1.00000000e+00]\n",
      "\n",
      "First few weights: [-0.2634089   0.219802   -0.4314835  -0.04742432 -0.00108816 -0.00303492\n",
      "  0.3918741  -0.28134894  0.11968399  0.33086663]\n",
      "\n",
      "Layer: quant_dense_2\n",
      "Shape: (1584, 24)\n",
      "Unique values: [-1.         -0.9949671  -0.9906221  ...  0.87707144  0.91965646\n",
      "  1.        ]\n",
      "\n",
      "First few weights: [ 0.0370524   0.06456068  0.02911065  0.020255    0.00137389 -0.0473805\n",
      " -0.00710181  0.0857726  -0.01976208 -0.13536301]\n",
      "\n",
      "Layer: quant_dense_3\n",
      "Shape: (24, 2)\n",
      "Unique values: [-7.4778354e-01 -7.4481171e-01 -6.6980577e-01 -6.1217695e-01\n",
      " -5.7736456e-01 -5.7269287e-01 -5.6762195e-01 -5.6652993e-01\n",
      " -5.6507009e-01 -5.2667886e-01 -4.9182498e-01 -4.7494432e-01\n",
      " -4.7409800e-01 -3.9295542e-01 -3.1159246e-01 -2.5039360e-01\n",
      " -2.4898832e-01 -1.4023753e-01 -8.1119649e-02 -6.2134523e-02\n",
      " -1.2550218e-03 -3.9678538e-04  1.9716497e-03  2.3040262e-03\n",
      "  3.7082434e-03  3.3313107e-02  4.9663808e-02  5.8584411e-02\n",
      "  6.4232394e-02  8.3540998e-02  1.2116299e-01  1.2440450e-01\n",
      "  1.5035811e-01  1.5638089e-01  2.3471943e-01  2.8484502e-01\n",
      "  3.0650625e-01  3.2876059e-01  3.3418477e-01  4.1253796e-01\n",
      "  4.7651157e-01  5.3438598e-01  5.7897133e-01  5.8447027e-01\n",
      "  6.7160791e-01  6.8456614e-01  7.9130352e-01  9.3726510e-01]\n",
      "\n",
      "First few weights: [ 3.3313107e-02 -1.2550218e-03 -3.9678538e-04  1.5638089e-01\n",
      " -7.4778354e-01  4.7651157e-01 -5.7269287e-01  3.3418477e-01\n",
      " -4.7409800e-01  1.2440450e-01]\n",
      "\n",
      "Layer: quant_conv2d_3, Weight shape: (2, 2, 1, 6), Size: 256 bytes\n",
      "Layer: quant_conv2d_4, Weight shape: (2, 2, 6, 4), Size: 544 bytes\n",
      "Layer: quant_conv2d_5, Weight shape: (2, 2, 4, 8), Size: 672 bytes\n",
      "Layer: quant_dense_2, Weight shape: (1584, 24), Size: 152192 bytes\n",
      "Layer: quant_dense_3, Weight shape: (24, 2), Size: 320 bytes\n",
      "Total memory usage: 153984 bytes\n"
     ]
    }
   ],
   "source": [
    "def display_weights(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, lq.layers.QuantConv2D) or isinstance(layer, lq.layers.QuantDense):\n",
    "            weights = layer.get_weights()\n",
    "            if weights:\n",
    "                print(f\"Layer: {layer.name}\")\n",
    "                for weight in weights:\n",
    "                    print(f\"Shape: {weight.shape}\")\n",
    "                    # Print the unique values to see if they are binarized\n",
    "                    unique_values = np.unique(weight)\n",
    "                    print(f\"Unique values: {unique_values}\\n\")\n",
    "                    # Print the first few weights for inspection\n",
    "                    print(f\"First few weights: {weight.flatten()[:10]}\\n\")\n",
    "\n",
    "# Display the weights of the binarized CNN model\n",
    "\n",
    "\n",
    "def model_memory_usage(model):\n",
    "    total_size = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (lq.layers.QuantConv2D, lq.layers.QuantDense)):\n",
    "            weights = layer.get_weights()\n",
    "            for weight in weights:\n",
    "                size = sys.getsizeof(weight)\n",
    "                total_size += size\n",
    "                print(f\"Layer: {layer.name}, Weight shape: {weight.shape}, Size: {size} bytes\")\n",
    "\n",
    "    print(f\"Total memory usage: {total_size} bytes\")\n",
    "\n",
    "# Display the weights of the binarized CNN model\n",
    "display_weights(model_3_20_epochs)\n",
    "\n",
    "# Check the memory usage of the model\n",
    "model_memory_usage(model_3_20_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show binary weights of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lq.context.quantized_scope(True):\n",
    "    model_3_20_epochs.save(\"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model_binary_weights.keras\")  # save binarized weights h5\n",
    "    weights = model_3_20_epochs.get_weights()  # get binarized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_weights_model = tf.keras.models.load_model(\"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model_binary_weights.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: quant_conv2d_3\n",
      "Shape: (2, 2, 1, 6)\n",
      "Unique values: [-1.  1.]\n",
      "\n",
      "First few weights: [-1.  1.  1.  1. -1.  1.  1. -1.  1.  1.]\n",
      "\n",
      "Layer: quant_conv2d_4\n",
      "Shape: (2, 2, 6, 4)\n",
      "Unique values: [-1.  1.]\n",
      "\n",
      "First few weights: [ 1. -1.  1. -1. -1.  1. -1. -1. -1.  1.]\n",
      "\n",
      "Layer: quant_conv2d_5\n",
      "Shape: (2, 2, 4, 8)\n",
      "Unique values: [-1.  1.]\n",
      "\n",
      "First few weights: [-1.  1. -1. -1. -1. -1.  1. -1.  1.  1.]\n",
      "\n",
      "Layer: quant_dense_2\n",
      "Shape: (1584, 24)\n",
      "Unique values: [-1.  1.]\n",
      "\n",
      "First few weights: [ 1.  1.  1.  1.  1. -1. -1.  1. -1. -1.]\n",
      "\n",
      "Layer: quant_dense_3\n",
      "Shape: (24, 2)\n",
      "Unique values: [-1.  1.]\n",
      "\n",
      "First few weights: [ 1. -1. -1.  1. -1.  1. -1.  1. -1.  1.]\n",
      "\n",
      "Layer: quant_conv2d_3, Weight shape: (2, 2, 1, 6), Size: 256 bytes\n",
      "Layer: quant_conv2d_4, Weight shape: (2, 2, 6, 4), Size: 544 bytes\n",
      "Layer: quant_conv2d_5, Weight shape: (2, 2, 4, 8), Size: 672 bytes\n",
      "Layer: quant_dense_2, Weight shape: (1584, 24), Size: 152192 bytes\n",
      "Layer: quant_dense_3, Weight shape: (24, 2), Size: 320 bytes\n",
      "Total memory usage: 153984 bytes\n"
     ]
    }
   ],
   "source": [
    "# Display the weights of the binarized CNN model\n",
    "display_weights(binarized_weights_model)\n",
    "\n",
    "# Check the memory usage of the model\n",
    "model_memory_usage(binarized_weights_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file name:  ../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model_binary_weights.keras\n",
      "File size: 0.519 Megabytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Model file name: \", \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model_binary_weights.keras\")\n",
    "convert_bytes(get_file_size(\"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_3_conv_layer_model_binary_weights.keras\"), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### larq TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmposuvsexo/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmposuvsexo/assets\n",
      "2024-07-26 20:54:52.588535: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmposuvsexo\n",
      "2024-07-26 20:54:52.597099: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-07-26 20:54:52.597183: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmposuvsexo\n",
      "2024-07-26 20:54:52.620085: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-07-26 20:54:52.749427: E external/org_tensorflow/tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute debug_name which is not in the op definition: Op<name=VarHandleOp; signature= -> resource:resource; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; attr=dtype:type; attr=shape:shape; attr=allowed_devices:list(string),default=[]; is_stateful=true> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node Adam/m/batch_normalization_5/beta}}\n",
      "2024-07-26 20:54:52.758368: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmposuvsexo\n",
      "2024-07-26 20:54:52.804191: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 215770 microseconds.\n",
      "2024-07-26 20:54:52.964512: I external/org_tensorflow/tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2138] Estimated count of arithmetic ops: 1.015 M  ops, equivalently 0.507 M  MACs\n"
     ]
    }
   ],
   "source": [
    "lce_model = lce.convert_keras_model(model_3_20_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check time of prdiction by bnn tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_time of one prediction, s:  0.00257986424298122\n"
     ]
    }
   ],
   "source": [
    "exec_time = []\n",
    "y_pred_all = []\n",
    "for i in range(1, len(x_test_np)):\n",
    "    # print(i)\n",
    "    start_time = time.time()\n",
    "    interpreter = lce.testing.Interpreter(lce_model)\n",
    "    y_pred_prob = interpreter.predict(x_test_np[i-1:i], verbose=0)\n",
    "    y_pred = tf.argmax(y_pred_prob, axis=1).numpy()\n",
    "    y_pred_all.extend(y_pred)\n",
    "    stop_time = time.time() - start_time\n",
    "    # print(stop_time)\n",
    "    exec_time.append(stop_time)\n",
    "print(\"mean_time of one prediction, s: \", sum(exec_time) / len(exec_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.04%\n",
      "Recall: 97.32%\n",
      "Precision: 87.31%\n",
      "F1-score: 92.04%\n"
     ]
    }
   ],
   "source": [
    "evaluate_prediction(y_pred_all, y_test_np[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write model in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159780"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_LITE_LARQ_MODEL_FILE_NAME = \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_lq.tflite\"\n",
    "open(TF_LITE_LARQ_MODEL_FILE_NAME, \"wb\").write(lce_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=TF_LITE_LARQ_MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont know how to read this model from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpklmkwoke/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpklmkwoke/assets\n",
      "2024-07-26 20:38:56.250224: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-26 20:38:56.250294: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-26 20:38:56.250459: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpklmkwoke\n",
      "2024-07-26 20:38:56.252873: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-26 20:38:56.252910: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpklmkwoke\n",
      "2024-07-26 20:38:56.260456: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-26 20:38:56.321188: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpklmkwoke\n",
      "2024-07-26 20:38:56.344950: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 94491 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 18, Total Ops 49, % non-converted = 36.73 %\n",
      " * 18 ARITH ops\n",
      "\n",
      "- arith.constant:   18 occurrences  (f32: 17, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 3)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n",
      "  (f32: 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset:\n",
      "Basic assessment of the whole dataset (without any partitions):\n",
      "Accuracy: 94.97%\n",
      "Recall: 87.12%\n",
      "Precision: 97.32%\n",
      "F1-score: 91.94%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9197318103358368\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9120115881644462\n",
      "\n",
      "Time for Test dataset:\n",
      "Accuracy: 94.97%\n",
      "Recall: 87.12%\n",
      "Precision: 97.32%\n",
      "F1-score: 91.94%\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.001 seconds\n",
      "Max: 0.002 seconds\n",
      "Min: 0.001 seconds\n",
      "\n",
      "\n",
      "Model file name:  ../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_tf_lite.tflite\n",
      "File size: 157.906 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(binarized_weights_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "(\n",
    "    y_pred_test, \n",
    "    non_overlap_patritions_f1_scores_test, \n",
    "    bootstrap_patritions_f1_scores_test,\n",
    "    ) = predict_and_print_full_results(tflite_model, x_test_np, y_test_np, model_format=\"tf_lite\")\n",
    "\n",
    "print(\"\\nTime for Test dataset:\")\n",
    "evaluate_time_of_prediction(tflite_model, x_test_np, y_test_np, model_format=\"tf_lite\")\n",
    "\n",
    "TF_LITE_MODEL_FILE_NAME = \"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_tf_lite.tflite\"\n",
    "open(TF_LITE_MODEL_FILE_NAME, \"wb\").write(tflite_model)\n",
    "print(\"\\n\")\n",
    "print(\"Model file name: \", TF_LITE_MODEL_FILE_NAME)\n",
    "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See tflite.tflite model weights and size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: serving_default_quant_conv2d_3_input:0, Index: 0, Shape: [  1 184  80   1], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_8/batchnorm/mul, Index: 1, Shape: [  24 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_9/batchnorm/mul, Index: 2, Shape: [ 2 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D, Index: 3, Shape: [6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D1, Index: 4, Shape: [6 2 2 1], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D, Index: 5, Shape: [4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D1, Index: 6, Shape: [4 2 2 6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D, Index: 7, Shape: [8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D1, Index: 8, Shape: [8 2 2 4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_9/batchnorm/sub, Index: 9, Shape: [2], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_8/batchnorm/sub, Index: 10, Shape: [24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/flatten_1/Const, Index: 11, Shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D/ste_sign_9/add/y, Index: 12, Shape: [], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV3, Index: 13, Shape: [6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV31, Index: 14, Shape: [6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV3, Index: 15, Shape: [4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV31, Index: 16, Shape: [4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV3, Index: 17, Shape: [8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV31, Index: 18, Shape: [8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D2, Index: 19, Shape: [  1 183  79   6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/max_pooling2d_3/MaxPool, Index: 20, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV32, Index: 21, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV33, Index: 22, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/Sign, Index: 23, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/add, Index: 24, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/Sign_1, Index: 25, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D2, Index: 26, Shape: [ 1 90 38  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/max_pooling2d_4/MaxPool, Index: 27, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV32, Index: 28, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV33, Index: 29, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/Sign, Index: 30, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/add, Index: 31, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/Sign_1, Index: 32, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D2, Index: 33, Shape: [ 1 44 18  8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/max_pooling2d_5/MaxPool, Index: 34, Shape: [ 1 22  9  8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV32, Index: 35, Shape: [ 1 22  9  8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV33, Index: 36, Shape: [ 1 22  9  8], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/flatten_1/Reshape, Index: 37, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/Sign, Index: 38, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/add, Index: 39, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/Sign_1, Index: 40, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/MatMul;sequential_1/batch_normalization_8/batchnorm/mul;sequential_1/batch_normalization_8/batchnorm/add_1, Index: 41, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/Sign, Index: 42, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/add, Index: 43, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/Sign_1, Index: 44, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/MatMul;sequential_1/batch_normalization_9/batchnorm/mul;sequential_1/batch_normalization_9/batchnorm/add_1, Index: 45, Shape: [1 2], dtype: <class 'numpy.float32'>\n",
      "Name: StatefulPartitionedCall:0, Index: 46, Shape: [1 2], dtype: <class 'numpy.float32'>\n",
      "Name: Conv_hwcn_weights, Index: 59, Shape: [4 6], dtype: <class 'numpy.float32'>\n",
      "Name: Conv_hwcn_weights, Index: 60, Shape: [24  4], dtype: <class 'numpy.float32'>\n",
      "Name: Conv_hwcn_weights, Index: 61, Shape: [16  8], dtype: <class 'numpy.float32'>\n",
      "Tensor serving_default_quant_conv2d_3_input:0 - Shape: (1, 184, 80, 1), Data: [[[[-0.9373283 ]\n",
      "   [-1.8462163 ]\n",
      "   [-1.6846098 ]\n",
      "   ...\n",
      "   [-2.871213  ]\n",
      "   [-3.8945668 ]\n",
      "   [-5.710342  ]]\n",
      "\n",
      "  [[-1.0653405 ]\n",
      "   [-0.8649982 ]\n",
      "   [-1.4073898 ]\n",
      "   ...\n",
      "   [-3.0824087 ]\n",
      "   [-3.8587742 ]\n",
      "   [-5.62705   ]]\n",
      "\n",
      "  [[-0.47574413]\n",
      "   [-0.50857115]\n",
      "   [-1.51533   ]\n",
      "   ...\n",
      "   [-3.0004165 ]\n",
      "   [-3.781811  ]\n",
      "   [-5.376748  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.6431216 ]\n",
      "   [-1.2507148 ]\n",
      "   [-0.7224055 ]\n",
      "   ...\n",
      "   [-3.381774  ]\n",
      "   [-3.6054962 ]\n",
      "   [-5.234658  ]]\n",
      "\n",
      "  [[-1.1810825 ]\n",
      "   [-1.051295  ]\n",
      "   [-1.2631068 ]\n",
      "   ...\n",
      "   [-3.219378  ]\n",
      "   [-3.9600673 ]\n",
      "   [-5.6320443 ]]\n",
      "\n",
      "  [[-0.70521295]\n",
      "   [-0.55519056]\n",
      "   [-1.4383278 ]\n",
      "   ...\n",
      "   [-2.8993685 ]\n",
      "   [-4.168313  ]\n",
      "   [-5.770375  ]]]]\n",
      "Tensor sequential_1/batch_normalization_8/batchnorm/mul - Shape: (24, 1584), Data: [[ 0.00354157  0.00354157 -0.00354157 ... -0.00354157  0.00354157\n",
      "  -0.00354157]\n",
      " [ 0.00800448  0.00800448  0.00800448 ...  0.00800448 -0.00800448\n",
      "   0.00800448]\n",
      " [ 0.00873648  0.00873648 -0.00873648 ...  0.00873648 -0.00873648\n",
      "  -0.00873648]\n",
      " ...\n",
      " [-0.00331745 -0.00331745 -0.00331745 ... -0.00331745 -0.00331745\n",
      "  -0.00331745]\n",
      " [ 0.01147186 -0.01147186  0.01147186 ... -0.01147186 -0.01147186\n",
      "   0.01147186]\n",
      " [-0.0089049   0.0089049  -0.0089049  ...  0.0089049  -0.0089049\n",
      "   0.0089049 ]]\n",
      "Tensor sequential_1/batch_normalization_9/batchnorm/mul - Shape: (2, 24), Data: [[ 0.08980736 -0.08980736 -0.08980736 -0.08980736 -0.08980736  0.08980736\n",
      "  -0.08980736  0.08980736  0.08980736 -0.08980736  0.08980736  0.08980736\n",
      "   0.08980736 -0.08980736 -0.08980736  0.08980736  0.08980736 -0.08980736\n",
      "   0.08980736 -0.08980736  0.08980736 -0.08980736 -0.08980736 -0.08980736]\n",
      " [-0.08745821  0.08745821  0.08745821  0.08745821  0.08745821 -0.08745821\n",
      "   0.08745821 -0.08745821  0.08745821  0.08745821  0.08745821 -0.08745821\n",
      "   0.08745821  0.08745821  0.08745821 -0.08745821 -0.08745821  0.08745821\n",
      "  -0.08745821  0.08745821 -0.08745821 -0.08745821  0.08745821  0.08745821]]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D - Shape: (6,), Data: [0. 0. 0. 0. 0. 0.]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D1 - Shape: (6, 2, 2, 1), Data: [[[[-1.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[ 1.]\n",
      "   [ 1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.]\n",
      "   [-1.]]\n",
      "\n",
      "  [[-1.]\n",
      "   [-1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[ 1.]\n",
      "   [-1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[-1.]\n",
      "   [ 1.]]]\n",
      "\n",
      "\n",
      " [[[-1.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[-1.]\n",
      "   [ 1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.]\n",
      "   [ 1.]]\n",
      "\n",
      "  [[-1.]\n",
      "   [ 1.]]]]\n",
      "Tensor sequential_1/quant_conv2d_4/QuantConv2D - Shape: (4,), Data: [0. 0. 0. 0.]\n",
      "Tensor sequential_1/quant_conv2d_4/QuantConv2D1 - Shape: (4, 2, 2, 6), Data: [[[[ 1. -1. -1.  1. -1.  1.]\n",
      "   [-1.  1.  1. -1. -1. -1.]]\n",
      "\n",
      "  [[-1.  1.  1. -1. -1. -1.]\n",
      "   [-1.  1. -1. -1. -1. -1.]]]\n",
      "\n",
      "\n",
      " [[[-1.  1.  1.  1.  1.  1.]\n",
      "   [-1. -1.  1. -1.  1.  1.]]\n",
      "\n",
      "  [[ 1.  1.  1. -1. -1.  1.]\n",
      "   [ 1. -1. -1.  1.  1.  1.]]]\n",
      "\n",
      "\n",
      " [[[ 1. -1. -1.  1.  1.  1.]\n",
      "   [ 1.  1. -1. -1.  1. -1.]]\n",
      "\n",
      "  [[ 1.  1. -1.  1.  1.  1.]\n",
      "   [ 1.  1. -1. -1.  1. -1.]]]\n",
      "\n",
      "\n",
      " [[[-1. -1. -1. -1.  1.  1.]\n",
      "   [ 1.  1. -1. -1.  1.  1.]]\n",
      "\n",
      "  [[-1.  1.  1. -1.  1.  1.]\n",
      "   [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "Tensor sequential_1/quant_conv2d_5/QuantConv2D - Shape: (8,), Data: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Tensor sequential_1/quant_conv2d_5/QuantConv2D1 - Shape: (8, 2, 2, 4), Data: [[[[-1.  1. -1. -1.]\n",
      "   [ 1. -1.  1. -1.]]\n",
      "\n",
      "  [[-1. -1.  1. -1.]\n",
      "   [ 1. -1.  1. -1.]]]\n",
      "\n",
      "\n",
      " [[[ 1.  1. -1.  1.]\n",
      "   [ 1. -1.  1. -1.]]\n",
      "\n",
      "  [[ 1.  1. -1. -1.]\n",
      "   [ 1. -1.  1. -1.]]]\n",
      "\n",
      "\n",
      " [[[-1.  1.  1.  1.]\n",
      "   [ 1.  1.  1.  1.]]\n",
      "\n",
      "  [[-1.  1.  1.  1.]\n",
      "   [ 1.  1.  1.  1.]]]\n",
      "\n",
      "\n",
      " [[[-1. -1. -1. -1.]\n",
      "   [-1.  1. -1.  1.]]\n",
      "\n",
      "  [[ 1.  1. -1. -1.]\n",
      "   [-1.  1. -1.  1.]]]\n",
      "\n",
      "\n",
      " [[[-1.  1.  1.  1.]\n",
      "   [ 1.  1.  1.  1.]]\n",
      "\n",
      "  [[ 1. -1.  1.  1.]\n",
      "   [ 1. -1.  1.  1.]]]\n",
      "\n",
      "\n",
      " [[[-1.  1. -1.  1.]\n",
      "   [-1. -1. -1. -1.]]\n",
      "\n",
      "  [[-1.  1.  1.  1.]\n",
      "   [-1.  1. -1. -1.]]]\n",
      "\n",
      "\n",
      " [[[ 1. -1. -1.  1.]\n",
      "   [ 1. -1. -1. -1.]]\n",
      "\n",
      "  [[-1.  1. -1. -1.]\n",
      "   [ 1. -1.  1. -1.]]]\n",
      "\n",
      "\n",
      " [[[-1. -1. -1. -1.]\n",
      "   [-1. -1. -1. -1.]]\n",
      "\n",
      "  [[-1. -1. -1. -1.]\n",
      "   [ 1.  1.  1.  1.]]]]\n",
      "Tensor sequential_1/batch_normalization_9/batchnorm/sub - Shape: (2,), Data: [ 0.08902183 -0.05441982]\n",
      "Tensor sequential_1/batch_normalization_8/batchnorm/sub - Shape: (24,), Data: [-1.0700091   0.5799124  -0.744154   -0.74155945 -1.1468663   0.972546\n",
      " -0.7318057   0.89624804  1.1183951  -0.8465897  -0.69563204  1.3483243\n",
      " -1.7116337  -1.0150051  -0.7908413   0.6676947   0.888582   -0.0271609\n",
      "  1.0740863   0.1314838   0.27894354 -0.9693073  -0.84099734 -0.98748267]\n",
      "Tensor sequential_1/flatten_1/Const - Shape: (2,), Data: [  -1 1584]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D/ste_sign_9/add/y - Shape: (), Data: 0.10000000149011612\n",
      "Tensor sequential_1/batch_normalization_5/FusedBatchNormV3 - Shape: (6,), Data: [0.18362285 0.18916714 0.18326458 0.18380241 1.1453308  0.18380241]\n",
      "Tensor sequential_1/batch_normalization_5/FusedBatchNormV31 - Shape: (6,), Data: [ 0.4076755  -0.6083193   1.3900635   0.42609882 -1.1537237  -0.1966489 ]\n",
      "Tensor sequential_1/batch_normalization_6/FusedBatchNormV3 - Shape: (4,), Data: [0.18471202 0.21667424 0.26398584 0.27221507]\n",
      "Tensor sequential_1/batch_normalization_6/FusedBatchNormV31 - Shape: (4,), Data: [-0.53293633 -0.13558313  0.2457928  -0.28320175]\n",
      "Tensor sequential_1/batch_normalization_7/FusedBatchNormV3 - Shape: (8,), Data: [0.26202005 0.23281957 0.15210299 0.21800128 0.2209664  0.1917535\n",
      " 0.21043523 0.27318588]\n",
      "Tensor sequential_1/batch_normalization_7/FusedBatchNormV31 - Shape: (8,), Data: [-0.28088677 -0.30893022 -0.5812032  -1.0656488   0.11313617  0.18977976\n",
      "  0.00789011 -0.42487222]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor data is null. Run allocate_tensors() first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract weights\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detail \u001b[38;5;129;01min\u001b[39;00m tensor_details:\n\u001b[0;32m---> 12\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newname/.venv/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:926\u001b[0m, in \u001b[0;36mInterpreter.tensor.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index):\n\u001b[1;32m    879\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns function that gives a numpy view of the current tensor buffer.\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \n\u001b[1;32m    881\u001b[0m \u001b[38;5;124;03m  This allows reading and writing to this tensors w/o copies. This more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    but it is not safe to hold the numpy array forever.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor data is null. Run allocate_tensors() first"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"../spectrogram_models_from_notebooks/bnn/hpo/bnn_mel_spec_tf_lite.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get tensor details\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "for detail in tensor_details:\n",
    "    print(f\"Name: {detail['name']}, Index: {detail['index']}, Shape: {detail['shape']}, dtype: {detail['dtype']}\")\n",
    "\n",
    "# Extract weights\n",
    "for detail in tensor_details:\n",
    "    tensor = interpreter.tensor(detail['index'])()\n",
    "    print(f\"Tensor {detail['name']} - Shape: {tensor.shape}, Data: {tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make full int quantization of tflite.tflite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptz1_mhcj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptz1_mhcj/assets\n",
      "/home/polina/newname/.venv/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-07-26 20:41:42.414186: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-07-26 20:41:42.414244: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-07-26 20:41:42.414455: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmptz1_mhcj\n",
      "2024-07-26 20:41:42.424287: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-07-26 20:41:42.424323: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmptz1_mhcj\n",
      "2024-07-26 20:41:42.434023: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-07-26 20:41:42.501602: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmptz1_mhcj\n",
      "2024-07-26 20:41:42.526481: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 112026 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 18, Total Ops 49, % non-converted = 36.73 %\n",
      " * 18 ARITH ops\n",
      "\n",
      "- arith.constant:   18 occurrences  (f32: 17, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 3)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n",
      "  (f32: 8)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(x_val_np).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "full_int_converter = tf.lite.TFLiteConverter.from_keras_model(binarized_weights_model)\n",
    "full_int_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "full_int_converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "full_int_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "full_int_converter.inference_input_type = tf.uint8\n",
    "full_int_converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = full_int_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: serving_default_quant_conv2d_3_input:0, Index: 0, Shape: [  1 184  80   1], dtype: <class 'numpy.uint8'>\n",
      "Name: sequential_1/flatten_1/Const, Index: 1, Shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/batch_normalization_9/batchnorm/sub, Index: 2, Shape: [2], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/batch_normalization_9/batchnorm/mul, Index: 3, Shape: [ 2 24], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_8/batchnorm/sub, Index: 4, Shape: [24], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/batch_normalization_8/batchnorm/mul, Index: 5, Shape: [  24 1584], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV31, Index: 6, Shape: [8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV3, Index: 7, Shape: [8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D, Index: 8, Shape: [8], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D1, Index: 9, Shape: [8 2 2 4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV31, Index: 10, Shape: [4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV3, Index: 11, Shape: [4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D, Index: 12, Shape: [4], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D1, Index: 13, Shape: [4 2 2 6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D/ste_sign_9/add/y, Index: 14, Shape: [], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV31, Index: 15, Shape: [6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV3, Index: 16, Shape: [6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D, Index: 17, Shape: [6], dtype: <class 'numpy.int32'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D1, Index: 18, Shape: [6 2 2 1], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.quantize, Index: 19, Shape: [  1 184  80   1], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_3/QuantConv2D2, Index: 20, Shape: [  1 183  79   6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/max_pooling2d_3/MaxPool, Index: 21, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV32, Index: 22, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_5/FusedBatchNormV33, Index: 23, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize, Index: 24, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/Sign, Index: 25, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize1, Index: 26, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/add, Index: 27, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize1, Index: 28, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_4/ste_sign_11/Sign_1, Index: 29, Shape: [ 1 91 39  6], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize2, Index: 30, Shape: [ 1 91 39  6], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_4/QuantConv2D2, Index: 31, Shape: [ 1 90 38  4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/max_pooling2d_4/MaxPool, Index: 32, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV32, Index: 33, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_6/FusedBatchNormV33, Index: 34, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize2, Index: 35, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/Sign, Index: 36, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize3, Index: 37, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/add, Index: 38, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize3, Index: 39, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_conv2d_5/ste_sign_13/Sign_1, Index: 40, Shape: [ 1 45 19  4], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize4, Index: 41, Shape: [ 1 45 19  4], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_conv2d_5/QuantConv2D2, Index: 42, Shape: [ 1 44 18  8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/max_pooling2d_5/MaxPool, Index: 43, Shape: [ 1 22  9  8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV32, Index: 44, Shape: [ 1 22  9  8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/batch_normalization_7/FusedBatchNormV33, Index: 45, Shape: [ 1 22  9  8], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/flatten_1/Reshape, Index: 46, Shape: [   1 1584], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/flatten_1/Reshape1, Index: 47, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/Sign, Index: 48, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize5, Index: 49, Shape: [   1 1584], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/add, Index: 50, Shape: [   1 1584], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize4, Index: 51, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_2/ste_sign_15/Sign_1, Index: 52, Shape: [   1 1584], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize6, Index: 53, Shape: [   1 1584], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_dense_2/MatMul;sequential_1/batch_normalization_8/batchnorm/mul;sequential_1/batch_normalization_8/batchnorm/add_1, Index: 54, Shape: [ 1 24], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize5, Index: 55, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/Sign, Index: 56, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize7, Index: 57, Shape: [ 1 24], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/add, Index: 58, Shape: [ 1 24], dtype: <class 'numpy.int8'>\n",
      "Name: tfl.dequantize6, Index: 59, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: sequential_1/quant_dense_3/ste_sign_17/Sign_1, Index: 60, Shape: [ 1 24], dtype: <class 'numpy.float32'>\n",
      "Name: tfl.quantize8, Index: 61, Shape: [ 1 24], dtype: <class 'numpy.int8'>\n",
      "Name: sequential_1/quant_dense_3/MatMul;sequential_1/batch_normalization_9/batchnorm/mul;sequential_1/batch_normalization_9/batchnorm/add_1, Index: 62, Shape: [1 2], dtype: <class 'numpy.int8'>\n",
      "Name: StatefulPartitionedCall:01, Index: 63, Shape: [1 2], dtype: <class 'numpy.int8'>\n",
      "Name: StatefulPartitionedCall:0, Index: 64, Shape: [1 2], dtype: <class 'numpy.uint8'>\n",
      "Name: , Index: 77, Shape: [  1 183  79   4], dtype: <class 'numpy.int8'>\n",
      "Name: , Index: 78, Shape: [ 1 90 38 24], dtype: <class 'numpy.int8'>\n",
      "Name: , Index: 79, Shape: [ 1 44 18 16], dtype: <class 'numpy.int8'>\n",
      "Tensor serving_default_quant_conv2d_3_input:0 - Shape: (1, 184, 80, 1), Data: [[[[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   ...\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[ 15]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   ...\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[113]\n",
      "   [117]\n",
      "   [ 97]\n",
      "   ...\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 58]\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   ...\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   [188]]\n",
      "\n",
      "  [[ 58]\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   ...\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   [ 60]]\n",
      "\n",
      "  [[ 58]\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   ...\n",
      "   [ 37]\n",
      "   [  3]\n",
      "   [188]]]]\n",
      "Tensor sequential_1/flatten_1/Const - Shape: (2,), Data: [  -1 1584]\n",
      "Tensor sequential_1/batch_normalization_9/batchnorm/sub - Shape: (2,), Data: [16051 -9812]\n",
      "Tensor sequential_1/batch_normalization_9/batchnorm/mul - Shape: (2, 24), Data: [[ 127 -127 -127 -127 -127  127 -127  127  127 -127  127  127  127 -127\n",
      "  -127  127  127 -127  127 -127  127 -127 -127 -127]\n",
      " [-124  124  124  124  124 -124  124 -124  124  124  124 -124  124  124\n",
      "   124 -124 -124  124 -124  124 -124 -124  124  124]]\n",
      "Tensor sequential_1/batch_normalization_8/batchnorm/sub - Shape: (24,), Data: [-1354999   734368  -942354  -939069 -1452326  1231577  -926717  1134958\n",
      "  1416272 -1072073  -880909  1707441 -2167516 -1285345 -1001477   845531\n",
      "  1125250   -34395  1360162   166504   353238 -1227476 -1064991 -1250492]\n",
      "Tensor sequential_1/batch_normalization_8/batchnorm/mul - Shape: (24, 1584), Data: [[  35   35  -35 ...  -35   35  -35]\n",
      " [  80   80   80 ...   80  -80   80]\n",
      " [  87   87  -87 ...   87  -87  -87]\n",
      " ...\n",
      " [ -33  -33  -33 ...  -33  -33  -33]\n",
      " [ 114 -114  114 ... -114 -114  114]\n",
      " [ -88   88  -88 ...   88  -88   88]]\n",
      "Tensor sequential_1/batch_normalization_7/FusedBatchNormV31 - Shape: (8,), Data: [  31   25  -30 -128  111  127   90    2]\n",
      "Tensor sequential_1/batch_normalization_7/FusedBatchNormV3 - Shape: (8,), Data: [117  89  14  75  78  51  68 127]\n",
      "Tensor sequential_1/quant_conv2d_5/QuantConv2D - Shape: (8,), Data: [0 0 0 0 0 0 0 0]\n",
      "Tensor sequential_1/quant_conv2d_5/QuantConv2D1 - Shape: (8, 2, 2, 4), Data: [[[[-127  127 -127 -127]\n",
      "   [ 127 -127  127 -127]]\n",
      "\n",
      "  [[-127 -127  127 -127]\n",
      "   [ 127 -127  127 -127]]]\n",
      "\n",
      "\n",
      " [[[ 127  127 -127  127]\n",
      "   [ 127 -127  127 -127]]\n",
      "\n",
      "  [[ 127  127 -127 -127]\n",
      "   [ 127 -127  127 -127]]]\n",
      "\n",
      "\n",
      " [[[-127  127  127  127]\n",
      "   [ 127  127  127  127]]\n",
      "\n",
      "  [[-127  127  127  127]\n",
      "   [ 127  127  127  127]]]\n",
      "\n",
      "\n",
      " [[[-127 -127 -127 -127]\n",
      "   [-127  127 -127  127]]\n",
      "\n",
      "  [[ 127  127 -127 -127]\n",
      "   [-127  127 -127  127]]]\n",
      "\n",
      "\n",
      " [[[-127  127  127  127]\n",
      "   [ 127  127  127  127]]\n",
      "\n",
      "  [[ 127 -127  127  127]\n",
      "   [ 127 -127  127  127]]]\n",
      "\n",
      "\n",
      " [[[-127  127 -127  127]\n",
      "   [-127 -127 -127 -127]]\n",
      "\n",
      "  [[-127  127  127  127]\n",
      "   [-127  127 -127 -127]]]\n",
      "\n",
      "\n",
      " [[[ 127 -127 -127  127]\n",
      "   [ 127 -127 -127 -127]]\n",
      "\n",
      "  [[-127  127 -127 -127]\n",
      "   [ 127 -127  127 -127]]]\n",
      "\n",
      "\n",
      " [[[-127 -127 -127 -127]\n",
      "   [-127 -127 -127 -127]]\n",
      "\n",
      "  [[-127 -127 -127 -127]\n",
      "   [ 127  127  127  127]]]]\n",
      "Tensor sequential_1/batch_normalization_6/FusedBatchNormV31 - Shape: (4,), Data: [-128    3  127  -46]\n",
      "Tensor sequential_1/batch_normalization_6/FusedBatchNormV3 - Shape: (4,), Data: [ 45  75 119 127]\n",
      "Tensor sequential_1/quant_conv2d_4/QuantConv2D - Shape: (4,), Data: [0 0 0 0]\n",
      "Tensor sequential_1/quant_conv2d_4/QuantConv2D1 - Shape: (4, 2, 2, 6), Data: [[[[ 127 -127 -127  127 -127  127]\n",
      "   [-127  127  127 -127 -127 -127]]\n",
      "\n",
      "  [[-127  127  127 -127 -127 -127]\n",
      "   [-127  127 -127 -127 -127 -127]]]\n",
      "\n",
      "\n",
      " [[[-127  127  127  127  127  127]\n",
      "   [-127 -127  127 -127  127  127]]\n",
      "\n",
      "  [[ 127  127  127 -127 -127  127]\n",
      "   [ 127 -127 -127  127  127  127]]]\n",
      "\n",
      "\n",
      " [[[ 127 -127 -127  127  127  127]\n",
      "   [ 127  127 -127 -127  127 -127]]\n",
      "\n",
      "  [[ 127  127 -127  127  127  127]\n",
      "   [ 127  127 -127 -127  127 -127]]]\n",
      "\n",
      "\n",
      " [[[-127 -127 -127 -127  127  127]\n",
      "   [ 127  127 -127 -127  127  127]]\n",
      "\n",
      "  [[-127  127  127 -127  127  127]\n",
      "   [ 127  127  127  127  127  127]]]]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D/ste_sign_9/add/y - Shape: (), Data: 127\n",
      "Tensor sequential_1/batch_normalization_5/FusedBatchNormV31 - Shape: (6,), Data: [  29  -73  127   31 -128  -32]\n",
      "Tensor sequential_1/batch_normalization_5/FusedBatchNormV3 - Shape: (6,), Data: [-87 -86 -87 -87 127 -87]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D - Shape: (6,), Data: [0 0 0 0 0 0]\n",
      "Tensor sequential_1/quant_conv2d_3/QuantConv2D1 - Shape: (6, 2, 2, 1), Data: [[[[-127]\n",
      "   [ 127]]\n",
      "\n",
      "  [[ 127]\n",
      "   [ 127]]]\n",
      "\n",
      "\n",
      " [[[ 127]\n",
      "   [-127]]\n",
      "\n",
      "  [[-127]\n",
      "   [-127]]]\n",
      "\n",
      "\n",
      " [[[ 127]\n",
      "   [ 127]]\n",
      "\n",
      "  [[ 127]\n",
      "   [-127]]]\n",
      "\n",
      "\n",
      " [[[ 127]\n",
      "   [ 127]]\n",
      "\n",
      "  [[-127]\n",
      "   [ 127]]]\n",
      "\n",
      "\n",
      " [[[-127]\n",
      "   [ 127]]\n",
      "\n",
      "  [[-127]\n",
      "   [ 127]]]\n",
      "\n",
      "\n",
      " [[[ 127]\n",
      "   [ 127]]\n",
      "\n",
      "  [[-127]\n",
      "   [ 127]]]]\n",
      "Tensor tfl.quantize - Shape: (1, 184, 80, 1), Data: [[[[-52]\n",
      "   [127]\n",
      "   [ 81]\n",
      "   ...\n",
      "   [127]\n",
      "   [ 81]\n",
      "   [ 60]]\n",
      "\n",
      "  [[-52]\n",
      "   [127]\n",
      "   [ 81]\n",
      "   ...\n",
      "   [127]\n",
      "   [ 81]\n",
      "   [-68]]\n",
      "\n",
      "  [[-52]\n",
      "   [127]\n",
      "   [ 81]\n",
      "   ...\n",
      "   [127]\n",
      "   [ 81]\n",
      "   [ 60]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 93]\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   ...\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   [ 60]]\n",
      "\n",
      "  [[ 93]\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   ...\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   [-68]]\n",
      "\n",
      "  [[ 93]\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   ...\n",
      "   [-38]\n",
      "   [ 25]\n",
      "   [ 60]]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor data is null. Run allocate_tensors() first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract weights\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detail \u001b[38;5;129;01min\u001b[39;00m tensor_details:\n\u001b[0;32m---> 12\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newname/.venv/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:926\u001b[0m, in \u001b[0;36mInterpreter.tensor.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index):\n\u001b[1;32m    879\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns function that gives a numpy view of the current tensor buffer.\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \n\u001b[1;32m    881\u001b[0m \u001b[38;5;124;03m  This allows reading and writing to this tensors w/o copies. This more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    but it is not safe to hold the numpy array forever.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor data is null. Run allocate_tensors() first"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors\n",
    "# interpreter = tf.lite.Interpreter(model_content=dynamic_range_quant_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get tensor details\n",
    "tensor_details = interpreter.get_tensor_details()\n",
    "for detail in tensor_details:\n",
    "    print(f\"Name: {detail['name']}, Index: {detail['index']}, Shape: {detail['shape']}, dtype: {detail['dtype']}\")\n",
    "\n",
    "# Extract weights\n",
    "for detail in tensor_details:\n",
    "    tensor = interpreter.tensor(detail['index'])()\n",
    "    print(f\"Tensor {detail['name']} - Shape: {tensor.shape}, Data: {tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51152"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import time\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"../spectrogram_models_from_notebooks/bnn/hpo\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the quantized model:\n",
    "tflite_bnn_full_int_file = tflite_models_dir/\"bnn_mel_spec_full_int_q.tflite\"\n",
    "tflite_bnn_full_int_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset:\n",
      "Accuracy: 96.09%\n",
      "Recall: 90.89%\n",
      "Precision: 97.50%\n",
      "F1-score: 94.08%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9395347080536413\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9404414092106499\n",
      "\n",
      "Test dataset:\n",
      "Accuracy: 95.19%\n",
      "Recall: 88.21%\n",
      "Precision: 96.88%\n",
      "F1-score: 92.34%\n",
      "\n",
      "Devide dataset into 10 non-overlapping patritions and get their mean F1-score\n",
      "Non-overlap mean F1-score:  0.9222558877595886\n",
      "\n",
      "Get 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\n",
      "Bootstrap mean F1-score:  0.9209436254661992\n",
      "\n",
      "Time for Test dataset:\n",
      "\n",
      "Time to make a prediction for a single data point\n",
      "Mean: 0.002 seconds\n",
      "Max: 0.012 seconds\n",
      "Min: 0.002 seconds\n",
      "File size: 49.953 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation dataset:\")\n",
    "predictions = full_int_model_predict(tflite_bnn_full_int_file, x_val_np)\n",
    "evaluate_prediction(y_val_np, predictions)\n",
    "\n",
    "print(\"\\nDevide dataset into 10 non-overlapping patritions and get their mean F1-score\")\n",
    "non_overlap_patritions_f1_scores = get_f1_scores_of_non_overlapping_partitions_full_int_q(tflite_bnn_full_int_file, x_val_np, y_val_np)\n",
    "print(\"Non-overlap mean F1-score: \", np.mean(non_overlap_patritions_f1_scores))\n",
    "\n",
    "print(\"\\nGet 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\")\n",
    "bootstrap_patritions_f1_scores = get_f1_scores_of_bootstarping_partitions_full_int_q(tflite_bnn_full_int_file, x_val_np, y_val_np)\n",
    "print(\"Bootstrap mean F1-score: \", np.mean(bootstrap_patritions_f1_scores))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "predictions = full_int_model_predict(tflite_bnn_full_int_file, x_test_np)\n",
    "evaluate_prediction(y_test_np, predictions)\n",
    "\n",
    "print(\"\\nDevide dataset into 10 non-overlapping patritions and get their mean F1-score\")\n",
    "non_overlap_patritions_f1_scores = get_f1_scores_of_non_overlapping_partitions_full_int_q(tflite_bnn_full_int_file, x_test_np, y_test_np)\n",
    "print(\"Non-overlap mean F1-score: \", np.mean(non_overlap_patritions_f1_scores))\n",
    "\n",
    "print(\"\\nGet 100 bootstrap samples from dataset with 100 samples each and get their mean F1-score\")\n",
    "bootstrap_patritions_f1_scores = get_f1_scores_of_bootstarping_partitions_full_int_q(tflite_bnn_full_int_file, x_test_np, y_test_np)\n",
    "print(\"Bootstrap mean F1-score: \", np.mean(bootstrap_patritions_f1_scores))\n",
    "\n",
    "print(\"\\nTime for Test dataset:\")\n",
    "time_data = []\n",
    "for data_point in x_test_np:\n",
    "    start_time = time.time()\n",
    "    predictions = full_int_model_predict(tflite_bnn_full_int_file, [data_point])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    time_data.append(elapsed_time)\n",
    "print(\"\\nTime to make a prediction for a single data point\")\n",
    "print(f\"Mean: {round(np.mean(time_data), 3)} seconds\")\n",
    "print(f\"Max: {round(np.max(time_data), 3)} seconds\")\n",
    "print(f\"Min: {round(np.min(time_data), 3)} seconds\")\n",
    "\n",
    "convert_bytes(get_file_size(tflite_bnn_full_int_file), \"KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that weights from -1 +1 changed to some different numbers in full int quant model. \n",
    "But model was quantized to full int with no losses in f1-score and even better (but weights from -1 and +1 becan -127 +127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
