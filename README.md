To achieve the goal of the thesis, we implement different Neural Network (NN) architectures such as CNN, SqueezeNet, tiny Transformer and BNN, characterized by a small size, and apply the Post-Training Quantization (PTQ) technique to them. Additionally, we investigate the performance of these models with both log Mel spectrograms and raw signal time-series as inputs. We also explore the possibility of these models being deployable on TinyML devices in terms of memory consumption and latency.

All model files are located in folder "models"

In "notebooks" folder the development of all architectures is presented.

In "notebooks.additional.results_with_calibrated_thresholds.ipynb" evaluation results of the models are presented.
